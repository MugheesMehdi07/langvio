# Table of Contents
- langvio/default_config.yaml
- langvio/config.py
- langvio/__init__.py
- langvio/cli.py
- langvio/utils/llm_utils.py
- langvio/utils/logging.py
- langvio/utils/__init__.py
- langvio/utils/vision_utils.py
- langvio/utils/file_utils.py
- langvio/vision/utils.py
- langvio/vision/base.py
- langvio/vision/color_detection.py
- langvio/vision/__init__.py
- langvio/media/processor.py
- langvio/media/__init__.py
- langvio/core/base.py
- langvio/core/registry.py
- langvio/core/__init__.py
- langvio/core/pipeline.py
- langvio/llm/base.py
- langvio/llm/google.py
- langvio/llm/openai.py
- langvio/llm/__init__.py
- langvio/llm/factory.py
- langvio/prompts/templates.py
- langvio/prompts/__init__.py
- langvio/prompts/constants.py
- langvio/vision/yolo/detector.py
- langvio/vision/yolo/__init__.py
- langvio/vision/yolo/yolo11_utils.py

## File: langvio/default_config.yaml

- Extension: .yaml
- Language: yaml
- Size: 1625 bytes
- Created: 2025-05-17 13:53:07
- Modified: 2025-05-17 13:53:07

### Code

```yaml
# Default langvio configuration
# Note: API keys should be provided through environment variables

llm:
  default: "gemini"
  models:
    gemini:
      model_name: "gemini-2.0-flash"
      model_kwargs:
        temperature: 0.2
        max_tokens: 1024

    gpt-3:
      model_name: "gpt-3.5-turbo"
      model_kwargs:
        temperature: 0.0
        max_tokens: 1024

    gpt-4:
      model_name: "gpt-4-turbo"
      model_kwargs:
        temperature: 0.1
        max_tokens: 2048

vision:
  default: "yolo"
  models:
    yoloe:
      type: "yolo"
      model_path: "yoloe-11s-seg-pf.pt"  # YOLOe nano - fastest
      confidence: 0.8
      model_type: "yoloe"

    yoloe_medium:
        type: "yolo"
        model_path: "yoloe-11m-seg-pf.pt"  # YOLOe medium - balanced
        confidence: 0.5
        model_type: "yoloe"

    yoloe_large:
        type: "yolo"
        model_path: "yoloe-11l-seg-pf.pt"  # YOLOe extra large - most accurate
        confidence: 0.5
        model_type: "yoloe"
    yolo:
      type: "yolo"
      model_path: "yolo11n.pt"  # Default: smallest/fastest model
      confidence: 0.5
    yolo_medium:
      type: "yolo"
      model_path: "yolo11m.pt"  # Medium model - balanced
      confidence: 0.5
    yolo_large:
      type: "yolo"
      model_path: "yolo11x.pt"  # Large model - most accurate
      confidence: 0.5

media:
  output_dir: "./output"
  temp_dir: "./temp"
  visualization:
    box_color: [0, 255, 0]  # Green boxes
    text_color: [255, 255, 255]  # White text
    line_thickness: 2
    show_attributes: true
    show_confidence: true

logging:
  level: "INFO"
  file: "langvio.log"
```

## File: langvio/config.py

- Extension: .py
- Language: python
- Size: 6245 bytes
- Created: 2025-04-20 23:39:14
- Modified: 2025-04-20 23:39:14

### Code

```python
"""
Configuration management for langvio
"""

import os
from typing import Any, Dict, Optional

import yaml
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env


class Config:
    """Configuration manager for langvio"""

    DEFAULT_CONFIG_PATH = os.path.join(os.path.dirname(__file__), "default_config.yaml")

    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize configuration.

        Args:
            config_path: Path to a YAML configuration file
        """
        # Initialize empty config
        self.config = {}

        # First load default config
        self._load_default_config()

        # Then load user config if provided
        if config_path and os.path.exists(config_path):
            self.load_config(config_path)

    def _load_default_config(self) -> None:
        """Load the default configuration from default_config.yaml"""
        try:
            if os.path.exists(self.DEFAULT_CONFIG_PATH):
                with open(self.DEFAULT_CONFIG_PATH, "r") as f:
                    self.config = yaml.safe_load(f)
                    if self.config is None:  # Handle empty file case
                        self.config = {}
            else:
                # Fallback default configuration if file doesn't exist
                self.config = {
                    "llm": {
                        "default": "gemini",
                        "models": {
                            "gemini": {
                                "model_name": "gemini-pro",
                                "model_kwargs": {"temperature": 0.2},
                            },
                            "gpt": {
                                "model_name": "gpt-3.5-turbo",
                                "model_kwargs": {"temperature": 0.0},
                            },
                        },
                    },
                    "vision": {
                        "default": "yolo",
                        "models": {
                            "yolo": {
                                "type": "yolo",
                                "model_path": "yolov11n.pt",
                                "confidence": 0.25,
                            }
                        },
                    },
                    "media": {
                        "output_dir": "./output",
                        "temp_dir": "./temp",
                        "visualization": {
                            "box_color": [0, 255, 0],
                            "text_color": [255, 255, 255],
                            "line_thickness": 2,
                        },
                    },
                    "logging": {"level": "INFO", "file": None},
                }
        except Exception as e:
            raise ValueError(f"Error loading default configuration: {e}")

    def load_config(self, config_path: str) -> None:
        """
        Load configuration from a YAML file.

        Args:
            config_path: Path to a YAML configuration file
        """
        try:
            with open(config_path, "r") as f:
                user_config = yaml.safe_load(f)
                if user_config is None:  # Handle empty file case
                    return

            # Update configuration
            self._update_config(self.config, user_config)
        except Exception as e:
            raise ValueError(f"Error loading configuration from {config_path}: {e}")

    def _update_config(
        self, base_config: Dict[str, Any], new_config: Dict[str, Any]
    ) -> None:
        """Recursively update base config with new config."""
        for key, value in new_config.items():
            if (
                isinstance(value, dict)
                and key in base_config
                and isinstance(base_config[key], dict)
            ):
                self._update_config(base_config[key], value)
            else:
                base_config[key] = value

    def get_llm_config(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """
        Get configuration for an LLM model.

        Args:
            model_name: Name of the model to get config for

        Returns:
            Model configuration dictionary
        """
        if not model_name:
            model_name = self.config["llm"]["default"]

        if model_name not in self.config["llm"]["models"]:
            raise ValueError(f"LLM model '{model_name}' not found in configuration")

        return self.config["llm"]["models"][model_name]

    def get_vision_config(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """
        Get configuration for a vision model.

        Args:
            model_name: Name of the model to get config for

        Returns:
            Model configuration dictionary
        """
        if not model_name:
            model_name = self.config["vision"]["default"]

        if model_name not in self.config["vision"]["models"]:
            raise ValueError(f"Vision model '{model_name}' not found in configuration")

        return self.config["vision"]["models"][model_name]

    def get_media_config(self) -> Dict[str, Any]:
        """
        Get media processing configuration.

        Returns:
            Media configuration dictionary
        """
        return self.config["media"]

    def get_logging_config(self) -> Dict[str, Any]:
        """
        Get logging configuration.

        Returns:
            Logging configuration dictionary
        """
        return self.config["logging"]

    def get_langsmith_config(self) -> Dict[str, Any]:
        """
        Get LangSmith configuration if available.

        Returns:
            LangSmith configuration dictionary
        """
        return self.config.get("langsmith", {})

    def save_config(self, config_path: str) -> None:
        """
        Save current configuration to a YAML file.

        Args:
            config_path: Path to save the configuration
        """
        try:
            with open(config_path, "w") as f:
                yaml.dump(self.config, f, default_flow_style=False)
        except Exception as e:
            raise ValueError(f"Error saving configuration to {config_path}: {e}")

```

## File: langvio/__init__.py

- Extension: .py
- Language: python
- Size: 3655 bytes
- Created: 2025-05-12 14:33:57
- Modified: 2025-05-12 14:33:57

### Code

```python
"""
langvio: Connect language models to vision models for natural language visual analysis
"""

__version__ = "0.3.0"

# Try to load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Initialize the global model registry
from langvio.core.registry import ModelRegistry
registry = ModelRegistry()

# Import main components for easier access
from langvio.core.pipeline import Pipeline
from langvio.llm.base import BaseLLMProcessor
from langvio.vision.base import BaseVisionProcessor

# Register YOLOe and YOLO processors
from langvio.vision.yolo.detector import YOLOProcessor

# Register the YOLO processor with different configurations
registry.register_vision_processor(
    "yolo",
    YOLOProcessor,
    model_path="yolo11n.pt",
    confidence=0.5
)

# Register the YOLOe processor with different sizes
registry.register_vision_processor(
    "yoloe",
    YOLOProcessor,
    model_path="yoloe-11s-seg-pf.pt",
    confidence=0.5,
    model_type="yoloe"
)

registry.register_vision_processor(
    "yoloe_medium",
    YOLOProcessor,
    model_path="yoloe-11m-seg-pf.pt",
    confidence=0.5,
    model_type="yoloe"
)

registry.register_vision_processor(
    "yoloe_large",
    YOLOProcessor,
    model_path="yoloe-11l-seg-pf.pt",
    confidence=0.5,
    model_type="yoloe"
)

# Register LLM processors using the factory
from langvio.llm.factory import register_llm_processors
register_llm_processors(registry)


# Default pipeline creator with better defaults
def create_pipeline(config_path=None, llm_name=None, vision_name=None):
    """
    Create a pipeline with optional configuration.

    Args:
        config_path: Path to a configuration file
        llm_name: Name of LLM processor to use
        vision_name: Name of vision processor to use (default: "yoloe")

    Returns:
        A configured Pipeline instance
    """
    import sys

    # Create the pipeline
    pipeline = Pipeline(config_path)

    # Set the vision processor (use YOLOe by default)
    if vision_name:
        pipeline.set_vision_processor(vision_name)
    else:
        # Default to YOLOe for best performance
        try:
            pipeline.set_vision_processor("yoloe_large")
        except:
            # Fall back to YOLO if YOLOe is not available
            pipeline.set_vision_processor("yolo")

    # Set the LLM processor
    if llm_name:
        # This will exit if the processor is not available
        pipeline.set_llm_processor(llm_name)
    else:
        # If no specific LLM is requested, try to use the default from config
        try:
            default_llm = pipeline.config.config["llm"]["default"]
            pipeline.set_llm_processor(default_llm)
        except Exception:
            # If we can't set a default LLM, check if any LLMs are available
            if len(registry.list_llm_processors()) == 0:
                error_msg = (
                    "ERROR: No LLM providers are installed. Please install at least one provider:\n"
                    "- For OpenAI: pip install langvio[openai]\n"
                    "- For Google Gemini: pip install langvio[google]\n"
                    "- For all providers: pip install langvio[all-llm]"
                )
                print(error_msg, file=sys.stderr)
                sys.exit(1)
            else:
                # Use the first available LLM
                available_llm = next(iter(registry.list_llm_processors()))
                pipeline.set_llm_processor(available_llm)

    return pipeline


# Version info and exports
__all__ = [
    "Pipeline",
    "create_pipeline",
    "registry",
    "BaseLLMProcessor",
    "BaseVisionProcessor",
]
```

## File: langvio/cli.py

- Extension: .py
- Language: python
- Size: 3674 bytes
- Created: 2025-04-20 23:47:11
- Modified: 2025-04-20 23:47:11

### Code

```python
"""
Command-line interface for langvio
"""

import argparse
import logging
import os
import sys

from langvio import create_pipeline
from langvio.utils.file_utils import is_image_file, is_video_file
from langvio.utils.logging import setup_logging


def main():
    """Main entry point for the langvio CLI"""
    # Parse arguments
    parser = argparse.ArgumentParser(
        description="langvio: Connect LLMs with video models"
    )

    # Required arguments
    parser.add_argument("--query", "-q", required=True, help="Natural language query")
    parser.add_argument(
        "--media", "-m", required=True, help="Path to image or video file"
    )

    # Optional arguments
    parser.add_argument("--config", "-c", help="Path to configuration file")
    parser.add_argument("--llm", "-l", help="LLM processor to use")
    parser.add_argument("--vision", "-v", help="Vision processor to use")
    parser.add_argument("--output", "-o", help="Output directory for processed media")
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Logging level",
    )
    parser.add_argument("--log-file", help="Path to log file")
    parser.add_argument(
        "--list-models", action="store_true", help="List available models and exit"
    )

    args = parser.parse_args()

    # Set up logging
    setup_logging({"level": args.log_level, "file": args.log_file})

    logger = logging.getLogger(__name__)

    # List models if requested
    if args.list_models:
        list_available_models()
        return 0

    # Check if media file exists
    if not os.path.exists(args.media):
        logger.error(f"Media file not found: {args.media}")
        return 1

    # Check if media file is supported
    if not is_image_file(args.media) and not is_video_file(args.media):
        logger.error(f"Unsupported media file format: {args.media}")
        return 1

    try:
        # Create pipeline
        pipeline = create_pipeline(
            config_path=args.config, llm_name=args.llm, vision_name=args.vision
        )

        # Update output directory if specified
        if args.output:
            pipeline.config.config["media"]["output_dir"] = args.output
            os.makedirs(args.output, exist_ok=True)

        # Process query
        result = pipeline.process(args.query, args.media)

        # Print results
        print("\n===== langvio Results =====")
        print(f"Query: {result['query']}")
        print(f"Media: {result['media_path']} ({result['media_type']})")
        print(f"Output: {result['output_path']}")
        print(f"\nExplanation: {result['explanation']}")
        print("\nDetection summary:")

        # Count detections by label
        counts = {}
        for frame_dets in result["detections"].values():
            for det in frame_dets:
                label = det["label"]
                counts[label] = counts.get(label, 0) + 1

        for label, count in counts.items():
            print(f"- {label}: {count}")

        print("\nProcessing complete!")
        return 0

    except Exception as e:
        logger.error(f"Error processing query: {e}", exc_info=True)
        return 1


def list_available_models():
    """List all available models in the registry"""
    from langvio import registry

    print("\n===== Available LLM Processors =====")
    for name, cls in registry.list_llm_processors().items():
        print(f"- {name}")

    print("\n===== Available Vision Processors =====")
    for name, cls in registry.list_vision_processors().items():
        print(f"- {name}")


if __name__ == "__main__":
    sys.exit(main())

```

## File: langvio/utils/llm_utils.py

- Extension: .py
- Language: python
- Size: 12603 bytes
- Created: 2025-05-05 20:07:54
- Modified: 2025-05-05 20:07:54

### Code

```python
"""
Enhanced utility functions for LLM processing with better YOLO11 metrics handling
"""

import json
import re
from typing import Any, Dict, List, Tuple


def index_detections(
    detections: Dict[str, List[Dict[str, Any]]],
) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Dict[str, Any]]]:
    """
    Create a copy of detections with unique object_id assigned to each detection.

    Args:
        detections: Original detection results

    Returns:
        Tuple of (indexed_detections, detection_map)
    """
    indexed_detections = {}
    detection_map = {}
    object_id_counter = 0

    for frame_key, frame_detections in detections.items():
        # Skip non-frame keys like "metrics" or "summary"
        if not frame_key.isdigit():
            indexed_detections[frame_key] = frame_detections
            continue

        indexed_detections[frame_key] = []

        for det in frame_detections:
            # Skip if not a dictionary
            if not isinstance(det, dict):
                continue

            # Create a copy with object_id
            object_id = f"obj_{object_id_counter}"
            det_copy = det.copy()
            det_copy["object_id"] = object_id

            # Add to indexed detections
            indexed_detections[frame_key].append(det_copy)

            # Add to object map with frame reference
            detection_map[object_id] = {
                "frame_key": frame_key,
                "detection": det,  # Store original detection
            }

            object_id_counter += 1

    return indexed_detections, detection_map


def format_detection_summary(
    detections: Dict[str, List[Dict[str, Any]]], query_params: Dict[str, Any]
) -> str:
    """
    Format the detection summary in a structured and readable way,
    including YOLO11 metrics if available.

    Args:
        detections: Dictionary with detection results and metrics
        query_params: Parsed query parameters

    Returns:
        Formatted detection summary string
    """
    summary_parts = []

    # Check if we have metrics or summary data from YOLO11
    has_metrics = "metrics" in detections
    has_summary = "summary" in detections

    # Add YOLO11 metrics summary if available
    if has_summary:
        summary = detections["summary"]

        # Add summary header
        summary_parts.append("# Summary Analysis")

        # Add object counts
        if "counts" in summary:
            summary_parts.append("\n## Object Counts")
            for label, count in sorted(summary["counts"].items(), key=lambda x: x[1], reverse=True):
                summary_parts.append(f"- {label}: {count} instances")

        # Add unique tracked objects for videos
        if "unique_tracked_objects" in summary:
            summary_parts.append(f"\n## Tracking Information")
            summary_parts.append(f"- Total unique objects tracked: {summary['unique_tracked_objects']}")

            if "unique_by_type" in summary:
                summary_parts.append("- Unique objects by type:")
                for label, count in sorted(summary["unique_by_type"].items(), key=lambda x: x[1], reverse=True):
                    summary_parts.append(f"  - {label}: {count} unique instances")

        # Add speed data if available
        if "speed" in summary:
            summary_parts.append("\n## Speed Information")
            # Format depends on the exact structure returned by YOLO11
            if isinstance(summary["speed"], dict):
                for obj_type, speed_data in summary["speed"].items():
                    if isinstance(speed_data, (int, float)):
                        summary_parts.append(f"- Average speed of {obj_type}: {speed_data:.1f} units/s")
                    elif isinstance(speed_data, dict) and "avg_speed" in speed_data:
                        summary_parts.append(f"- Average speed of {obj_type}: {speed_data['avg_speed']:.1f} units/s")

        # Add counting data if available
        if "counting" in summary:
            summary_parts.append("\n## Object Counting")
            counting_data = summary["counting"]
            if isinstance(counting_data, dict):
                for count_type, count_value in counting_data.items():
                    summary_parts.append(f"- {count_type}: {count_value}")

        # Add frame info for videos
        if "total_frames_analyzed" in summary:
            summary_parts.append(f"\n## Video Analysis")
            summary_parts.append(f"- Total frames analyzed: {summary['total_frames_analyzed']}")

        # Add a separator
        summary_parts.append("\n" + "-" * 40 + "\n")

    # Count objects by type from basic detections
    object_counts = {}
    object_details = []

    # Process the detection frames (skip metadata keys)
    frame_keys = [k for k in detections.keys() if k.isdigit()]

    if frame_keys:
        # For images (single frame) or videos (multiple frames)
        is_video = len(frame_keys) > 1

        if is_video:
            summary_parts.append(f"Analyzed {len(frame_keys)} frames of video")

        # Collect statistics and object details
        for frame_key in frame_keys:
            frame_prefix = f"Frame {frame_key}: " if is_video else ""
            frame_detections = detections[frame_key]

            for det in frame_detections:
                if not isinstance(det, dict):
                    continue

                label = det.get("label", "unknown")
                object_id = det.get("object_id", "unknown")

                # Update counts
                if label not in object_counts:
                    object_counts[label] = 0
                object_counts[label] += 1

                # Create detailed object entry
                obj_details = f"{frame_prefix}[{object_id}] {label}"

                # Add confidence
                if "confidence" in det:
                    obj_details += f" (confidence: {det['confidence']:.2f})"

                # Add attributes
                if "attributes" in det and det["attributes"]:
                    attrs = [f"{k}:{v}" for k, v in det["attributes"].items()]
                    obj_details += f" - {', '.join(attrs)}"

                # Add position if available
                if "position_area" in det:
                    obj_details += f" - position: {det['position_area']}"

                # Add activities for videos
                if "activities" in det and det["activities"]:
                    obj_details += f" - activities: {', '.join(det['activities'])}"

                # Add key relationships (limit to first 2 for readability)
                if "relationships" in det and det["relationships"]:
                    for rel in det["relationships"][:2]:
                        if "object" in rel and "relations" in rel and rel["relations"]:
                            obj_details += f" - {rel['relations'][0]} {rel['object']}"

                object_details.append(obj_details)

        # Add object counts to summary if not already added from summary data
        if not has_summary and object_counts:
            summary_parts.append("Basic Object Counts:")
            for label, count in sorted(object_counts.items(), key=lambda x: x[1], reverse=True):
                summary_parts.append(f"- {label}: {count} instances")

        # Add detailed object list (limited to avoid overwhelming the LLM)
        if object_details:
            summary_parts.append("\nDetailed Object List:")
            # Limit to 50 objects to keep context manageable
            for detail in object_details[:50]:
                summary_parts.append(f"- {detail}")

            if len(object_details) > 50:
                summary_parts.append(f"... and {len(object_details) - 50} more objects")

        # Check for target objects specified in the query
        if query_params.get("target_objects"):
            target_objects = query_params["target_objects"]
            summary_parts.append(
                f"\nTarget objects specified in query: {', '.join(target_objects)}"
            )
    else:
        # No detections available
        summary_parts.append("No detections available in the provided media")

    return "\n".join(summary_parts)


def extract_object_ids(highlight_text: str) -> List[str]:
    """
    Extract object IDs from highlight text, handling various formats.

    Args:
        highlight_text: Text containing object IDs to highlight

    Returns:
        List of object IDs
    """
    object_ids = []

    # Clean text
    cleaned_text = highlight_text.strip()

    # Try to parse as JSON array first
    if cleaned_text.startswith("[") and cleaned_text.endswith("]"):
        try:
            parsed_ids = json.loads(cleaned_text)
            if isinstance(parsed_ids, list):
                for item in parsed_ids:
                    if isinstance(item, str):
                        object_ids.append(item)
                    elif isinstance(item, dict) and "object_id" in item:
                        object_ids.append(item["object_id"])
                return object_ids
        except json.JSONDecodeError:
            pass

    # Regular expression to find object IDs (obj_X format)
    obj_pattern = r"obj_\d+"
    found_ids = re.findall(obj_pattern, cleaned_text)
    if found_ids:
        return found_ids

    # Look for any bracketed IDs
    bracket_pattern = r"\[([^\]]+)\]"
    bracket_matches = re.findall(bracket_pattern, cleaned_text)
    for match in bracket_matches:
        if match.startswith("obj_"):
            object_ids.append(match)

    # If still no IDs found, split by lines and look for obj_ prefix
    if not object_ids:
        lines = [line.strip() for line in cleaned_text.split("\n")]
        for line in lines:
            if line.startswith("obj_") or "obj_" in line:
                # Extract the obj_X part
                parts = line.split()
                for part in parts:
                    if part.startswith("obj_"):
                        # Remove any punctuation
                        clean_part = re.sub(r"[^\w_]", "", part)
                        object_ids.append(clean_part)

    return object_ids


def get_objects_by_ids(
    object_ids: List[str], detection_map: Dict[str, Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Get the actual detection objects by their IDs.

    Args:
        object_ids: List of object IDs to retrieve
        detection_map: Map of object_id to detection information

    Returns:
        List of detection objects with frame reference
    """
    result = []

    for obj_id in object_ids:
        if obj_id in detection_map:
            object_info = detection_map[obj_id]
            # Create a reference that includes both the detection and its frame
            result.append(
                {
                    "frame_key": object_info["frame_key"],
                    "detection": object_info["detection"],
                }
            )

    return result


def parse_explanation_response(
    response_content: str, detection_map: Dict[str, Dict[str, Any]]
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Parse the LLM response to extract explanation and highlighted objects.
    The explanation section will be cleaned to remove the highlighting instructions.

    Args:
        response_content: LLM response content
        detection_map: Map of object_id to detection information

    Returns:
        Tuple of (explanation_text, highlight_objects)
    """
    explanation_text = ""
    highlight_objects = []

    # Extract explanation and highlight sections
    parts = response_content.split("HIGHLIGHT_OBJECTS:")

    if len(parts) > 1:
        explanation_part = parts[0].strip()
        highlight_part = parts[1].strip()

        # Extract the explanation text (remove the EXPLANATION: prefix if present)
        if "EXPLANATION:" in explanation_part:
            explanation_text = explanation_part.split("EXPLANATION:", 1)[1].strip()
        else:
            explanation_text = explanation_part

        # Extract object IDs and get corresponding objects
        object_ids = extract_object_ids(highlight_part)
        highlight_objects = get_objects_by_ids(object_ids, detection_map)
    else:
        # If no highlight section found, use the whole response as explanation
        # but still try to clean it if it has the EXPLANATION: prefix
        if "EXPLANATION:" in response_content:
            explanation_text = response_content.split("EXPLANATION:", 1)[1].strip()
        else:
            explanation_text = response_content

    return explanation_text, highlight_objects
```

## File: langvio/utils/logging.py

- Extension: .py
- Language: python
- Size: 1447 bytes
- Created: 2025-04-20 23:39:14
- Modified: 2025-04-20 23:39:14

### Code

```python
"""
Logging utilities
"""

import logging
import os
from typing import Any, Dict, Optional


def setup_logging(config: Optional[Dict[str, Any]] = None) -> None:
    """
    Set up logging with configuration.

    Args:
        config: Logging configuration
    """
    if config is None:
        config = {"level": "INFO", "file": None}

    # Get log level
    level_name = config.get("level", "INFO").upper()
    level = getattr(logging, level_name, logging.INFO)

    # Basic configuration
    handlers = []

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    console_formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    console_handler.setFormatter(console_formatter)
    handlers.append(console_handler)

    # File handler if specified
    log_file = config.get("file")
    if log_file:
        # Create directory if needed
        log_dir = os.path.dirname(log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)

        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(level)
        file_formatter = logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
        file_handler.setFormatter(file_formatter)
        handlers.append(file_handler)

    # Configure root logger
    logging.basicConfig(level=level, handlers=handlers, force=True)

```

## File: langvio/utils/__init__.py

- Extension: .py
- Language: python
- Size: 0 bytes
- Created: 2025-04-01 07:38:54
- Modified: 2025-04-01 07:38:54

### Code

```python

```

## File: langvio/utils/vision_utils.py

- Extension: .py
- Language: python
- Size: 2691 bytes
- Created: 2025-04-26 23:02:51
- Modified: 2025-04-26 23:02:51

### Code

```python
"""
Utility functions for vision processing
"""

from typing import Any, Dict, List, Optional, Tuple

"""
Updated utility functions for vision processing
"""

from typing import Any, Dict, List, Optional, Tuple


def add_spatial_context(
    detections: List[Dict[str, Any]], dimensions: Optional[Tuple[int, int]]
) -> List[Dict[str, Any]]:
    """
    Add spatial context to detections (positions and relationships).

    Args:
        detections: List of detection dictionaries
        dimensions: Optional tuple of (width, height)

    Returns:
        Enhanced detections with spatial context
    """
    # Skip if no dimensions provided
    if not dimensions or not detections:
        return detections

    # Calculate relative positions
    from langvio.vision.utils import (
        calculate_relative_positions,
        detect_spatial_relationships,
    )

    # Add relative positions based on image dimensions
    detections = calculate_relative_positions(detections, *dimensions)

    # Add spatial relationships between objects
    detections = detect_spatial_relationships(detections)

    return detections


# Updated to work with highlighting
def create_visualization_detections_for_video(
    all_detections: Dict[str, List[Dict[str, Any]]],
    highlight_objects: List[Dict[str, Any]],
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Create a filtered detections dictionary for video visualization.
    NOTE: This function is now mainly kept for backward compatibility.
    With the new visualization approach, we use all_detections directly.

    Args:
        all_detections: Dictionary with all detection results
        highlight_objects: List of directly referenced objects to highlight

    Returns:
        Dictionary with filtered detection results for visualization
    """
    # With the new approach, we return all detections
    # The highlighting is done by checking against highlight_objects in the visualization
    return all_detections


# Updated to work with highlighting
def create_visualization_detections_for_image(
    highlight_objects: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    Create a list of detections for image visualization.
    NOTE: This function is now mainly kept for backward compatibility.
    With the new visualization approach, we keep all detections.

    Args:
        highlight_objects: List of directly referenced objects to highlight

    Returns:
        List of detection objects for highlighting
    """
    # Extract detection objects from highlight_objects
    return [
        obj_info.get("detection")
        for obj_info in highlight_objects
        if obj_info.get("detection") is not None
    ]
```

## File: langvio/utils/file_utils.py

- Extension: .py
- Language: python
- Size: 2788 bytes
- Created: 2025-04-20 23:38:43
- Modified: 2025-04-20 23:38:43

### Code

```python
"""
File handling utilities
"""

import os
import shutil
import tempfile
from typing import List, Optional


def ensure_directory(directory: str) -> None:
    """
    Ensure a directory exists.

    Args:
        directory: Directory path
    """
    os.makedirs(directory, exist_ok=True)


def get_file_extension(file_path: str) -> str:
    """
    Get the extension of a file.

    Args:
        file_path: Path to the file

    Returns:
        File extension (lowercase)
    """
    _, ext = os.path.splitext(file_path)
    return ext.lower()


def is_image_file(file_path: str) -> bool:
    """
    Check if a file is an image based on extension.

    Args:
        file_path: Path to the file

    Returns:
        True if the file is an image
    """
    image_extensions = [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".webp"]
    return get_file_extension(file_path) in image_extensions


def is_video_file(file_path: str) -> bool:
    """
    Check if a file is a video based on extension.

    Args:
        file_path: Path to the file

    Returns:
        True if the file is a video
    """
    video_extensions = [".mp4", ".avi", ".mov", ".mkv", ".webm"]
    return get_file_extension(file_path) in video_extensions


def create_temp_copy(file_path: str, delete: bool = True) -> str:
    """
    Create a temporary copy of a file.

    Args:
        file_path: Path to the original file
        delete: Whether to delete the temp file when Python exits

    Returns:
        Path to the temporary copy
    """
    # Create temp file with same extension
    ext = get_file_extension(file_path)
    temp_file = tempfile.NamedTemporaryFile(suffix=ext, delete=False)
    temp_path = temp_file.name
    temp_file.close()

    # Copy file
    shutil.copy2(file_path, temp_path)

    # Register for deletion if requested
    if delete:
        import atexit

        atexit.register(
            lambda: os.remove(temp_path) if os.path.exists(temp_path) else None
        )

    return temp_path


def get_files_in_directory(
    directory: str, extensions: Optional[List[str]] = None
) -> List[str]:
    """
    Get files in a directory with optional extension filtering.

    Args:
        directory: Directory path
        extensions: List of extensions to filter by

    Returns:
        List of file paths
    """
    if not os.path.isdir(directory):
        return []

    files = []

    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)

        if os.path.isfile(file_path):
            if extensions is None:
                files.append(file_path)
            else:
                file_ext = get_file_extension(file_path)
                if file_ext in extensions:
                    files.append(file_path)

    return files

```

## File: langvio/vision/utils.py

- Extension: .py
- Language: python
- Size: 9543 bytes
- Created: 2025-05-06 12:22:07
- Modified: 2025-05-06 12:22:07

### Code

```python
"""
Enhanced utilities for vision processing
"""

from typing import Any, Dict, List


# from langvio.prompts.constants import (
#     VISUAL_ATTRIBUTES,
#     SPATIAL_RELATIONS,
#     ACTIVITIES,
#     DEFAULT_IOU_THRESHOLD
# )
# Add to langvio/vision/yolo11_utils.py

def optimize_for_memory():
    """
    Set PyTorch memory optimization settings.
    """
    import os
    import torch

    # Set environment variables to reduce memory fragmentation
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    # Enable memory caching to reduce allocations
    torch.cuda.empty_cache()

    # Set to use TF32 precision if available (for Ampere and later GPUs)
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # Set inference mode for PyTorch
    torch.set_grad_enabled(False)

def extract_detections(results) -> List[Dict[str, Any]]:
    """
    Extract detections from YOLO results with enhanced attributes.

    Args:
        results: Raw YOLO results

    Returns:
        List of detection dictionaries with enhanced attributes
    """
    detections = []

    for result in results:
        boxes = result.boxes

        for box in boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            conf = float(box.conf[0])
            cls_id = int(box.cls[0])
            label = result.names[cls_id]

            # Calculate center point and dimensions for spatial relation analysis
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            width = x2 - x1
            height = y2 - y1
            area = width * height

            # Enhanced detection object with additional fields
            detections.append(
                {
                    "label": label,
                    "confidence": conf,
                    "bbox": [x1, y1, x2, y2],
                    "class_id": cls_id,
                    # Additional attributes for spatial and attribute analysis
                    "center": (center_x, center_y),
                    "dimensions": (width, height),
                    "area": area,
                    "relative_size": None,  # Will be calculated later based on image dimensions
                    "attributes": {},  # Placeholder for detected attributes
                    "activities": [],  # Placeholder for detected activities (for videos)
                    "relationships": [],  # Will be populated during relationship analysis
                }
            )

    return detections


def calculate_relative_positions(
    detections: List[Dict[str, Any]], image_width: int, image_height: int
) -> List[Dict[str, Any]]:
    """
    Calculate relative positions and sizes of detections.

    Args:
        detections: List of detection dictionaries
        image_width: Width of the image
        image_height: Height of the image

    Returns:
        Updated list of detections with relative position information
    """
    image_area = image_width * image_height

    for det in detections:
        # Calculate relative size compared to image
        det["relative_size"] = det["area"] / image_area

        # Calculate relative positions (0-1)
        center_x, center_y = det["center"]
        det["relative_position"] = (center_x / image_width, center_y / image_height)

        # Classify position in image (top-left, center, etc.)
        rx, ry = det["relative_position"]
        position = ""

        # Vertical position
        if ry < 0.33:
            position += "top-"
        elif ry < 0.66:
            position += "middle-"
        else:
            position += "bottom-"

        # Horizontal position
        if rx < 0.33:
            position += "left"
        elif rx < 0.66:
            position += "center"
        else:
            position += "right"

        det["position_area"] = position

    return detections


def detect_spatial_relationships(
    detections: List[Dict[str, Any]], distance_threshold: float = 0.2
) -> List[Dict[str, Any]]:
    """
    Detect spatial relationships between objects.

    Args:
        detections: List of detection dictionaries
        distance_threshold: Threshold for 'near' relationship (as fraction of image width)

    Returns:
        Updated list of detections with relationship information
    """
    if len(detections) < 2:
        return detections

    for i, det1 in enumerate(detections):
        for j, det2 in enumerate(detections):
            if i == j:
                continue

            # Get centers and boxes
            center1_x, center1_y = det1["center"]
            center2_x, center2_y = det2["center"]
            box1 = det1["bbox"]
            box2 = det2["bbox"]

            # Initialize relationship entry
            relationship = {"object": det2["label"], "object_id": j, "relations": []}

            # Check left/right
            if center1_x < center2_x:
                relationship["relations"].append("left_of")
            else:
                relationship["relations"].append("right_of")

            # Check above/below
            if center1_y < center2_y:
                relationship["relations"].append("above")
            else:
                relationship["relations"].append("below")

            # Check near
            distance = (
                (center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2
            ) ** 0.5
            if distance < distance_threshold * (
                det1["dimensions"][0] + det2["dimensions"][0]
            ):
                relationship["relations"].append("near")
            else:
                relationship["relations"].append("far")

            # Check inside/containing
            x1_1, y1_1, x2_1, y2_1 = box1
            x1_2, y1_2, x2_2, y2_2 = box2

            if x1_1 > x1_2 and y1_1 > y1_2 and x2_1 < x2_2 and y2_1 < y2_2:
                relationship["relations"].append("inside")
            elif x1_2 > x1_1 and y1_2 > y1_1 and x2_2 < x2_1 and y2_2 < y2_1:
                relationship["relations"].append("contains")

            # Add the relationship to the detection
            det1["relationships"].append(relationship)

    return detections




def filter_by_attributes(
    detections: List[Dict[str, Any]], required_attributes: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Filter detections by required attributes.

    Args:
        detections: List of detection dictionaries
        required_attributes: List of required attribute dictionaries
                            (e.g. [{"attribute": "color", "value": "red"}])

    Returns:
        Filtered list of detections
    """
    if not required_attributes:
        return detections

    filtered = []

    for det in detections:
        matches_all = True

        for req in required_attributes:
            attr_name = req.get("attribute")
            attr_value = req.get("value")

            # Skip if the attribute isn't specified
            if not attr_name or not attr_value:
                continue

            # Check if the detection has this attribute with matching value
            if (
                attr_name not in det["attributes"]
                or det["attributes"][attr_name] != attr_value
            ):
                matches_all = False
                break

        if matches_all:
            filtered.append(det)

    return filtered


def filter_by_spatial_relations(
    detections: List[Dict[str, Any]], required_relations: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Filter detections by required spatial relationships.

    Args:
        detections: List of detection dictionaries
        required_relations: List of required relationship dictionaries
                           (e.g. [{"relation": "above", "object": "table"}])

    Returns:
        Filtered list of detections
    """
    if not required_relations:
        return detections

    filtered = []

    for det in detections:
        should_include = True

        for req_rel in required_relations:
            relation_type = req_rel.get("relation")
            target_object = req_rel.get("object")

            # Skip if relation or target object isn't specified
            if not relation_type or not target_object:
                continue

            # Check if this detection has the required relationship
            has_relation = False

            for rel in det["relationships"]:
                if (
                    rel["object"].lower() == target_object.lower()
                    and relation_type in rel["relations"]
                ):
                    has_relation = True
                    break

            if not has_relation:
                should_include = False
                break

        if should_include:
            filtered.append(det)

    return filtered


def filter_by_activities(
    detections: List[Dict[str, Any]], required_activities: List[str]
) -> List[Dict[str, Any]]:
    """
    Filter detections by required activities.

    Args:
        detections: List of detection dictionaries
        required_activities: List of required activities (e.g. ["walking", "running"])

    Returns:
        Filtered list of detections
    """
    if not required_activities:
        return detections

    filtered = []

    for det in detections:
        for activity in required_activities:
            if activity.lower() in [a.lower() for a in det["activities"]]:
                filtered.append(det)
                break

    return filtered

```

## File: langvio/vision/base.py

- Extension: .py
- Language: python
- Size: 4857 bytes
- Created: 2025-05-06 12:24:34
- Modified: 2025-05-06 12:24:34

### Code

```python
"""
Enhanced base classes for vision processors
"""

from abc import abstractmethod
from typing import Any, Dict, List, Optional, Tuple

import cv2

from langvio.core.base import Processor
from langvio.prompts.constants import DEFAULT_VIDEO_SAMPLE_RATE
from langvio.vision.color_detection import ColorDetector



class BaseVisionProcessor(Processor):
    """Enhanced base class for all vision processors"""

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        """
        Initialize vision processor.

        Args:
            name: Processor name
            config: Configuration parameters
        """
        super().__init__(name, config)
        self.model = None

    @abstractmethod
    def process_image(
        self, image_path: str, query_params: Dict[str, Any]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Process an image with the vision model.

        Args:
            image_path: Path to the input image
            query_params: Parameters from the query processor

        Returns:
            Dictionary with detection results
        """

    @abstractmethod
    def process_video(
        self,
        video_path: str,
        query_params: Dict[str, Any],
        sample_rate: int = DEFAULT_VIDEO_SAMPLE_RATE,
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Process a video with the vision model.

        Args:
            video_path: Path to the input video
            query_params: Parameters from the query processor
            sample_rate: Process every Nth frame

        Returns:
            Dictionary with detection results
        """


    def _get_image_dimensions(self, image_path: str) -> Optional[Tuple[int, int]]:
        """
        Get dimensions of an image.

        Args:
            image_path: Path to the image

        Returns:
            Tuple of (width, height) or None if failed
        """
        try:
            image = cv2.imread(image_path)
            if image is not None:
                height, width = image.shape[:2]
                return (width, height)
        except Exception:
            pass
        return None

    def _enhance_detections_with_attributes(
        self, detections: List[Dict[str, Any]], image_path: str
    ) -> List[Dict[str, Any]]:
        """
        Enhance detections with attribute information.
        Subclasses can override this to add more sophisticated attribute detection.

        Args:
            detections: List of detection dictionaries
            image_path: Path to the image

        Returns:
            Detections with added attributes
        """
        # Load image
        try:
            image = cv2.imread(image_path)
            if image is None:
                return detections

            image_height, image_width = image.shape[:2]

            for det in detections:
                # Extract bounding box
                x1, y1, x2, y2 = det["bbox"]

                # Skip invalid boxes
                if (
                    x1 >= x2
                    or y1 >= y2
                    or x1 < 0
                    or y1 < 0
                    or x2 > image_width
                    or y2 > image_height
                ):
                    continue

                # Get the object region
                obj_region = image[y1:y2, x1:x2]

                # Initialize attributes dictionary if not present
                if "attributes" not in det:
                    det["attributes"] = {}

                # Calculate basic size attribute
                area = (x2 - x1) * (y2 - y1)
                image_area = image_width * image_height
                relative_size = area / image_area

                if relative_size < 0.05:
                    det["attributes"]["size"] = "small"
                elif relative_size < 0.25:
                    det["attributes"]["size"] = "medium"
                else:
                    det["attributes"]["size"] = "large"

                # Extract dominant color (very basic implementation)
                if obj_region.size > 0:
                    # Get color information
                    color_info = ColorDetector.get_color_profile(obj_region)

                    # Add to detection attributes
                    if "attributes" not in det:
                        det["attributes"] = {}

                    det["attributes"]["color"] = color_info["dominant_color"]
                    det["attributes"]["is_multicolored"] = color_info["is_multicolored"]

                    # Optionally add all detected colors
                    det["attributes"]["colors"] = list(
                        color_info["color_percentages"].keys()
                    )

        except Exception:
            # In case of any errors, return original detections
            pass

        return detections

```

## File: langvio/vision/color_detection.py

- Extension: .py
- Language: python
- Size: 13603 bytes
- Created: 2025-04-21 00:01:28
- Modified: 2025-04-21 00:01:28

### Code

```python
"""
Advanced color detection utility class
"""

from typing import Dict, List, Tuple, Union

import cv2
import numpy as np


class ColorDetector:
    """
    Advanced color detection utility class for image processing.
    Provides precise color detection with support for a wide range of color shades.

    This class can be integrated with any object detection system to add color attribute detection.
    """

    # Define comprehensive color ranges in HSV
    # Format: (lower_bound, upper_bound, color_name)
    # H: 0-180, S: 0-255, V: 0-255 for OpenCV
    COLOR_RANGES = [
        # Grayscale spectrum
        ((0, 0, 0), (180, 30, 50), "black"),
        ((0, 0, 51), (180, 30, 140), "dark_gray"),
        ((0, 0, 141), (180, 30, 200), "gray"),
        ((0, 0, 201), (180, 30, 255), "white"),
        # Red spectrum (wraps around the hue spectrum)
        ((0, 70, 50), (10, 255, 255), "red"),
        ((160, 70, 50), (180, 255, 255), "red"),
        ((0, 70, 50), (10, 150, 200), "dark_red"),
        ((160, 70, 50), (180, 150, 200), "dark_red"),
        ((0, 50, 200), (10, 150, 255), "light_red"),
        ((160, 50, 200), (180, 150, 255), "light_red"),
        # Pink spectrum
        ((145, 30, 190), (165, 120, 255), "pink"),
        ((145, 120, 190), (165, 255, 255), "hot_pink"),
        ((0, 30, 200), (10, 70, 255), "salmon"),
        ((160, 30, 200), (180, 70, 255), "salmon"),
        # Orange spectrum
        ((5, 120, 150), (25, 255, 255), "orange"),
        ((5, 150, 150), (18, 255, 255), "pure_orange"),
        ((18, 150, 150), (27, 255, 255), "amber"),
        ((10, 70, 150), (25, 150, 200), "bronze"),
        ((10, 100, 100), (25, 150, 150), "brown"),
        # Yellow spectrum
        ((25, 100, 150), (40, 255, 255), "yellow"),
        ((25, 150, 200), (40, 255, 255), "bright_yellow"),
        ((25, 100, 100), (40, 150, 150), "olive"),
        ((25, 50, 150), (40, 100, 200), "gold"),
        ((25, 30, 200), (40, 70, 255), "cream"),
        # Green spectrum
        ((40, 70, 50), (85, 255, 255), "green"),
        ((40, 150, 100), (70, 255, 200), "pure_green"),
        ((70, 150, 100), (85, 255, 200), "lime_green"),
        ((40, 100, 50), (70, 200, 100), "dark_green"),
        ((40, 50, 150), (70, 100, 200), "light_green"),
        ((35, 30, 70), (50, 80, 120), "olive_green"),
        # Cyan spectrum
        ((85, 70, 100), (105, 255, 255), "cyan"),
        ((85, 150, 150), (105, 255, 255), "bright_cyan"),
        ((85, 70, 100), (105, 150, 150), "teal"),
        ((85, 50, 150), (105, 100, 200), "turquoise"),
        # Blue spectrum
        ((105, 70, 50), (135, 255, 255), "blue"),
        ((105, 150, 100), (125, 255, 200), "pure_blue"),
        ((105, 100, 50), (125, 200, 100), "dark_blue"),
        ((105, 50, 150), (125, 100, 200), "light_blue"),
        ((125, 150, 100), (135, 255, 200), "royal_blue"),
        # Purple spectrum
        ((135, 70, 50), (160, 255, 255), "purple"),
        ((135, 150, 100), (150, 255, 200), "purple"),
        ((135, 100, 50), (150, 200, 100), "dark_purple"),
        ((135, 50, 150), (150, 100, 200), "light_purple"),
        ((140, 150, 100), (160, 255, 200), "violet"),
        ((135, 30, 100), (150, 70, 150), "lavender"),
        # Brown spectrum (additional)
        ((0, 50, 50), (20, 150, 120), "brown"),
        ((5, 50, 50), (25, 100, 100), "dark_brown"),
        ((5, 30, 100), (25, 70, 150), "light_brown"),
        ((10, 30, 120), (30, 70, 170), "tan"),
        ((10, 20, 140), (30, 50, 200), "beige"),
        # Metallic colors
        ((0, 0, 150), (180, 20, 200), "silver"),
        ((20, 30, 100), (40, 70, 150), "gold"),
    ]

    @classmethod
    def detect_color(
        cls, image_region: np.ndarray, return_all: bool = False, threshold: float = 0.15
    ) -> Union[str, Dict[str, float]]:
        """
        Detect the dominant color(s) in an image region using HSV color space.

        Args:
            image_region: Image region as numpy array (BGR format)
            return_all: If True, returns all detected colors with percentages
            threshold: Minimum percentage for a color to be considered (when return_all=False)

        Returns:
            Dominant color name if return_all=False, or dictionary of {color_name: percentage} if return_all=True
        """
        # Check if region is valid
        if (
            image_region is None
            or image_region.size == 0
            or image_region.shape[0] == 0
            or image_region.shape[1] == 0
        ):
            return "unknown" if not return_all else {}

        # Convert to HSV for better color analysis
        try:
            hsv_region = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
        except cv2.error:
            return "unknown" if not return_all else {}

        # Create a mask for each color range and count pixels
        color_counts = {}
        nonzero_pixels = np.count_nonzero(
            hsv_region[:, :, 0] >= 0
        )  # Total valid pixels

        if nonzero_pixels == 0:
            return "unknown" if not return_all else {}

        for lower, upper, color_name in cls.COLOR_RANGES:
            lower_bound = np.array(lower)
            upper_bound = np.array(upper)

            # Create mask for this color range
            mask = cv2.inRange(hsv_region, lower_bound, upper_bound)
            pixel_count = np.count_nonzero(mask)

            # Calculate percentage of pixels in this color range
            percentage = pixel_count / nonzero_pixels

            # Add to color counts if percentage is significant
            if (
                percentage > 0.05
            ):  # Only count colors covering at least 5% of the region
                if color_name in color_counts:
                    color_counts[color_name] += percentage
                else:
                    color_counts[color_name] = percentage

        # If return_all is True, return the full color dictionary
        if return_all:
            # Sort by percentage (highest first)
            return dict(sorted(color_counts.items(), key=lambda x: x[1], reverse=True))

        # Get the color with the highest percentage
        if not color_counts:
            return "unknown"

        dominant_color = max(color_counts.items(), key=lambda x: x[1])

        # Only return a color if it covers a significant portion of the object
        if dominant_color[1] >= threshold:  # At least threshold % of pixels
            return dominant_color[0]
        else:
            return "multicolored"  # No single dominant color

    @classmethod
    def detect_colors_layered(
        cls, image_region: np.ndarray, max_colors: int = 3
    ) -> List[str]:
        """
        Detect up to max_colors different colors in the image region in order of dominance.

        Args:
            image_region: Image region as numpy array (BGR format)
            max_colors: Maximum number of colors to return

        Returns:
            List of color names in order of dominance
        """
        # Get all colors with their percentages
        color_percentages = cls.detect_color(image_region, return_all=True)

        # Return the top colors
        return [color for color, _ in list(color_percentages.items())[:max_colors]]

    @classmethod
    def get_color_profile(cls, image_region: np.ndarray) -> Dict[str, any]:
        """
        Get a comprehensive color profile of the image region.

        Args:
            image_region: Image region as numpy array (BGR format)

        Returns:
            Dictionary with color information:
                - dominant_color: Main color name
                - color_percentages: Dictionary of all detected colors and their percentages
                - is_multicolored: Boolean indicating if the object has multiple significant colors
                - brightness: Average brightness value
                - saturation: Average saturation value
        """
        # Check if region is valid
        if image_region is None or image_region.size == 0:
            return {
                "dominant_color": "unknown",
                "color_percentages": {},
                "is_multicolored": False,
                "brightness": 0,
                "saturation": 0,
            }

        # Get all color percentages
        color_percentages = cls.detect_color(image_region, return_all=True)

        # Get dominant color
        dominant_color = "unknown"
        if color_percentages:
            dominant_color = max(color_percentages.items(), key=lambda x: x[1])[0]

        # Determine if multicolored (more than one color with significant percentage)
        significant_colors = [c for c, p in color_percentages.items() if p >= 0.2]
        is_multicolored = len(significant_colors) > 1

        # Compute average brightness and saturation
        try:
            hsv_region = cv2.cvtColor(image_region, cv2.COLOR_BGR2HSV)
            avg_saturation = np.mean(hsv_region[:, :, 1])
            avg_brightness = np.mean(hsv_region[:, :, 2])
        except:
            avg_saturation = 0
            avg_brightness = 0

        return {
            "dominant_color": dominant_color,
            "color_percentages": color_percentages,
            "is_multicolored": is_multicolored,
            "brightness": float(avg_brightness),
            "saturation": float(avg_saturation),
        }

    @classmethod
    def get_color_name(cls, bgr_color: Tuple[int, int, int]) -> str:
        """
        Get the name of a color given its BGR values.

        Args:
            bgr_color: Tuple of (Blue, Green, Red) values (0-255)

        Returns:
            Name of the closest matching color
        """
        # Convert single BGR color to a 1x1 image
        pixel = np.array([[[bgr_color[0], bgr_color[1], bgr_color[2]]]], dtype=np.uint8)
        return cls.detect_color(pixel)

    @classmethod
    def find_objects_by_color(cls, image: np.ndarray, target_color: str) -> np.ndarray:
        """
        Create a mask highlighting areas of the specified color in the image.

        Args:
            image: Input image as numpy array (BGR format)
            target_color: Color name to find

        Returns:
            Binary mask where areas of the target color are white (255)
        """
        # Check if image is valid
        if image is None or image.size == 0:
            return np.array([], dtype=np.uint8)

        # Convert to HSV
        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

        # Create combined mask for the target color
        combined_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)

        # Find all ranges matching the target color
        for lower, upper, color_name in cls.COLOR_RANGES:
            if color_name == target_color:
                mask = cv2.inRange(hsv_image, np.array(lower), np.array(upper))
                combined_mask = cv2.bitwise_or(combined_mask, mask)

        return combined_mask

    @classmethod
    def visualize_colors(cls, image_region: np.ndarray) -> np.ndarray:
        """
        Create a visualization of detected colors in the image region.

        Args:
            image_region: Image region as numpy array (BGR format)

        Returns:
            Visualization image with color information
        """
        if image_region is None or image_region.size == 0:
            return np.zeros((100, 200, 3), dtype=np.uint8)

        # Get color profile
        profile = cls.get_color_profile(image_region)

        # Create visualization image
        height, width = 30 * len(profile["color_percentages"]) + 60, 300
        vis_image = np.ones((height, width, 3), dtype=np.uint8) * 255

        # Add title
        cv2.putText(
            vis_image,
            "Color Analysis",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (0, 0, 0),
            2,
        )

        # Add color bars
        y_offset = 60
        for i, (color, percentage) in enumerate(profile["color_percentages"].items()):
            # Draw color name and percentage
            text = f"{color}: {percentage * 100:.1f}%"
            cv2.putText(
                vis_image,
                text,
                (10, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (0, 0, 0),
                1,
            )

            # Draw color bar
            bar_width = int(percentage * 150)

            # Try to use actual color for visualization
            try:
                # Find a matching color range
                for lower, upper, c_name in cls.COLOR_RANGES:
                    if c_name == color:
                        # Use the middle value of the range for visualization
                        h = (lower[0] + upper[0]) // 2
                        s = (lower[1] + upper[1]) // 2
                        v = (lower[2] + upper[2]) // 2

                        # Convert HSV to BGR for display
                        hsv_color = np.uint8([[[h, s, v]]])
                        bgr_color = cv2.cvtColor(hsv_color, cv2.COLOR_HSV2BGR)[0][0]
                        break
                else:
                    # Default color if no match found
                    bgr_color = (0, 0, 0)
            except:
                # Fallback color
                bgr_color = (0, 0, 0)

            cv2.rectangle(
                vis_image,
                (150, y_offset - 15),
                (150 + bar_width, y_offset),
                bgr_color.tolist(),
                -1,
            )

            y_offset += 30

        return vis_image

```

## File: langvio/vision/__init__.py

- Extension: .py
- Language: python
- Size: 0 bytes
- Created: 2025-04-01 07:38:54
- Modified: 2025-04-01 07:38:54

### Code

```python

```

## File: langvio/media/processor.py

- Extension: .py
- Language: python
- Size: 18074 bytes
- Created: 2025-04-26 23:04:09
- Modified: 2025-04-26 23:04:09

### Code

```python
"""
Enhanced media processing utilities
"""

import logging
import os
from typing import Any, Dict, List, Optional, Tuple, Union

import cv2
import numpy as np


class MediaProcessor:
    """Enhanced processor for handling media files (images and videos)"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize media processor.

        Args:
            config: Configuration parameters
        """
        self.config = config or {
            "output_dir": "./output",
            "temp_dir": "./temp",
            "visualization": {
                "box_color": [0, 255, 0],
                "text_color": [255, 255, 255],
                "line_thickness": 2,
                "show_attributes": True,
                "show_confidence": True,
            },
        }

        self.logger = logging.getLogger(__name__)

        # Create output and temp directories
        os.makedirs(self.config["output_dir"], exist_ok=True)
        os.makedirs(self.config["temp_dir"], exist_ok=True)

    def update_config(self, config: Dict[str, Any]) -> None:
        """
        Update configuration parameters.

        Args:
            config: New configuration parameters
        """
        self.config.update(config)

        # Ensure directories exist
        os.makedirs(self.config["output_dir"], exist_ok=True)
        os.makedirs(self.config["temp_dir"], exist_ok=True)

    def is_video(self, file_path: str) -> bool:
        """
        Check if a file is a video based on extension.

        Args:
            file_path: Path to the file

        Returns:
            True if the file is a video
        """
        video_extensions = [".mp4", ".avi", ".mov", ".mkv", ".webm"]
        _, ext = os.path.splitext(file_path.lower())
        return ext in video_extensions

    def get_output_path(self, input_path: str, suffix: str = "_processed") -> str:
        """
        Generate an output path for processed media.

        Args:
            input_path: Path to the input file
            suffix: Suffix to add to the filename

        Returns:
            Output path
        """
        filename = os.path.basename(input_path)
        name, ext = os.path.splitext(filename)
        output_filename = f"{name}{suffix}{ext}"

        return os.path.join(self.config["output_dir"], output_filename)

    def _get_color_for_id(self, track_id: int) -> Tuple[int, int, int]:
        """
        Generate a consistent color for a given track ID.

        Args:
            track_id: Track identifier

        Returns:
            BGR color tuple
        """
        # Use the track_id to generate repeatable colors
        hue = (track_id * 137 % 360) / 360.0  # Use prime number to distribute colors
        sat = 0.7 + (track_id % 3) * 0.1  # Vary saturation slightly
        val = 0.8 + (track_id % 2) * 0.2  # Vary value slightly

        # Convert HSV to RGB then to BGR
        import colorsys

        r, g, b = colorsys.hsv_to_rgb(hue, sat, val)
        r, g, b = int(r * 255), int(g * 255), int(b * 255)

        return (b, g, r)  # Return BGR for OpenCV

    def visualize_image_with_highlights(
            self,
            image_path: str,
            output_path: str,
            all_detections: List[Dict[str, Any]],
            highlighted_detections: List[Dict[str, Any]],
            original_box_color: Union[Tuple[int, int, int], List[int]] = (0, 255, 0),
            highlight_color: Union[Tuple[int, int, int], List[int]] = (0, 0, 255),
            text_color: Union[Tuple[int, int, int], List[int]] = (255, 255, 255),
            line_thickness: int = 2,
            show_attributes: bool = True,
            show_confidence: bool = True,
    ) -> None:
        """
        Visualize all detections on an image with highlighted objects in a different color.

        Args:
            image_path: Path to the input image
            output_path: Path to save the output image
            all_detections: List of all detection dictionaries
            highlighted_detections: List of detection dictionaries to highlight
            original_box_color: Color for non-highlighted bounding boxes (BGR)
            highlight_color: Color for highlighted bounding boxes (BGR)
            text_color: Color for text (BGR)
            line_thickness: Thickness of bounding box lines
            show_attributes: Whether to display attribute information
            show_confidence: Whether to display confidence scores
        """
        self.logger.info(
            f"Visualizing {len(all_detections)} detections on image: {image_path}"
        )

        try:
            # Load image
            image = cv2.imread(image_path)
            if image is None:
                raise ValueError(f"Failed to load image: {image_path}")

            # Create set of highlighted detections for quick lookup
            # Since we can't use the detection objects directly as dict keys,
            # we'll create a signature based on the bounding box and label
            highlighted_signatures = set()
            for det in highlighted_detections:
                if "bbox" in det and "label" in det:
                    # Create a signature that uniquely identifies this detection
                    signature = (
                        det["label"],
                        tuple(det["bbox"]) if isinstance(det["bbox"], list) else det["bbox"]
                    )
                    highlighted_signatures.add(signature)

            # Draw all detections with appropriate colors
            for det in all_detections:
                # Check if this detection is in the highlighted set
                is_highlighted = False
                if "bbox" in det and "label" in det:
                    signature = (
                        det["label"],
                        tuple(det["bbox"]) if isinstance(det["bbox"], list) else det["bbox"]
                    )
                    is_highlighted = signature in highlighted_signatures

                # Choose color based on whether the detection is highlighted
                box_color = highlight_color if is_highlighted else original_box_color

                # Use thicker lines for highlighted objects
                thickness = line_thickness + 1 if is_highlighted else line_thickness

                # Draw the detection with the chosen color and thickness
                image = self._draw_single_detection(
                    image,
                    det,
                    box_color,
                    text_color,
                    thickness,
                    show_attributes,
                    show_confidence,
                    is_highlighted
                )

            # Save output
            cv2.imwrite(output_path, image)
            self.logger.info(f"Saved visualized image to: {output_path}")
        except Exception as e:
            self.logger.error(f"Error visualizing image: {e}")

    def visualize_video_with_highlights(
            self,
            video_path: str,
            output_path: str,
            all_frame_detections: Dict[str, List[Dict[str, Any]]],
            highlighted_objects: List[Dict[str, Any]],
            original_box_color: Union[Tuple[int, int, int], List[int]] = (0, 255, 0),
            highlight_color: Union[Tuple[int, int, int], List[int]] = (0, 0, 255),
            text_color: Union[Tuple[int, int, int], List[int]] = (255, 255, 255),
            line_thickness: int = 2,
            show_attributes: bool = True,
            show_confidence: bool = True,
    ) -> None:
        """
        Visualize all detections on a video with highlighted objects in a different color.

        Args:
            video_path: Path to the input video
            output_path: Path to save the output video
            all_frame_detections: Dictionary mapping frame indices to all detections
            highlighted_objects: List of objects to highlight with frame references
            original_box_color: Color for non-highlighted bounding boxes (BGR)
            highlight_color: Color for highlighted bounding boxes (BGR)
            text_color: Color for text (BGR)
            line_thickness: Thickness of bounding box lines
            show_attributes: Whether to display attribute information
            show_confidence: Whether to display confidence scores
        """
        self.logger.info(f"Visualizing all detections on video: {video_path}")

        try:
            # Create a lookup for highlighted objects by frame
            highlighted_by_frame = {}

            for obj in highlighted_objects:
                frame_key = obj.get("frame_key")
                detection = obj.get("detection")

                if frame_key and detection:
                    if frame_key not in highlighted_by_frame:
                        highlighted_by_frame[frame_key] = []

                    highlighted_by_frame[frame_key].append(detection)

            # Open input video
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"Failed to open video: {video_path}")

            # Get video properties
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)

            # Create video writer
            fourcc = cv2.VideoWriter_fourcc(*"mp4v")
            writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

            # For tracking visualization - keep track of past positions
            tracks = {}  # Dictionary mapping track_id to list of past positions

            # Process frames
            frame_idx = 0

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                # Check if we have detections for this frame
                frame_key = str(frame_idx)
                if frame_key in all_frame_detections:
                    # Get current detections
                    all_detections = all_frame_detections[frame_key]

                    # Get highlighted detections for this frame
                    highlighted_detections = highlighted_by_frame.get(frame_key, [])

                    # Create set of highlighted detections for quick lookup
                    highlighted_signatures = set()
                    for det in highlighted_detections:
                        if "bbox" in det and "label" in det:
                            # Create a signature that uniquely identifies this detection
                            signature = (
                                det["label"],
                                tuple(det["bbox"]) if isinstance(det["bbox"], list) else det["bbox"]
                            )
                            highlighted_signatures.add(signature)

                    # Update tracks for visualization
                    for det in all_detections:
                        if "track_id" in det:
                            track_id = det["track_id"]
                            # Get center of bounding box
                            bbox = det["bbox"]
                            center = (
                                int((bbox[0] + bbox[2]) / 2),
                                int((bbox[1] + bbox[3]) / 2),
                            )

                            # Add to track history
                            if track_id not in tracks:
                                tracks[track_id] = []

                            # Keep only last 30 positions
                            if len(tracks[track_id]) > 30:
                                tracks[track_id] = tracks[track_id][-30:]

                            tracks[track_id].append(center)

                    # Draw trajectory lines for tracked objects
                    for track_id, positions in tracks.items():
                        if len(positions) > 1:
                            # Generate unique color for this track
                            track_color = self._get_color_for_id(track_id)

                            # Draw line connecting positions
                            for i in range(len(positions) - 1):
                                cv2.line(
                                    frame,
                                    positions[i],
                                    positions[i + 1],
                                    track_color,
                                    thickness=max(1, line_thickness - 1),
                                )

                    # Draw all detections with appropriate colors
                    for det in all_detections:
                        # Check if this detection is in the highlighted set
                        is_highlighted = False
                        if "bbox" in det and "label" in det:
                            signature = (
                                det["label"],
                                tuple(det["bbox"]) if isinstance(det["bbox"], list) else det["bbox"]
                            )
                            is_highlighted = signature in highlighted_signatures

                        # Choose color based on whether the detection is highlighted
                        box_color = highlight_color if is_highlighted else original_box_color

                        # Use thicker lines for highlighted objects
                        thickness = line_thickness + 1 if is_highlighted else line_thickness

                        # Draw the detection with the chosen color and thickness
                        frame = self._draw_single_detection(
                            frame,
                            det,
                            box_color,
                            text_color,
                            thickness,
                            show_attributes,
                            show_confidence,
                            is_highlighted
                        )

                # Write frame
                writer.write(frame)
                frame_idx += 1

            # Clean up
            cap.release()
            writer.release()
            self.logger.info(f"Saved visualized video to: {output_path}")
        except Exception as e:
            self.logger.error(f"Error visualizing video: {e}")

    def _draw_single_detection(
            self,
            image: np.ndarray,
            det: Dict[str, Any],
            box_color: Union[Tuple[int, int, int], List[int]],
            text_color: Union[Tuple[int, int, int], List[int]],
            line_thickness: int,
            show_attributes: bool,
            show_confidence: bool,
            is_highlighted: bool = False,
    ) -> np.ndarray:
        """
        Draw a single detection on an image.

        Args:
            image: Input image
            det: Detection dictionary
            box_color: Color for bounding box
            text_color: Color for text
            line_thickness: Thickness of bounding box lines
            show_attributes: Whether to show attribute information
            show_confidence: Whether to show confidence
            is_highlighted: Whether this is a highlighted detection

        Returns:
            Updated image
        """
        if "bbox" not in det:
            return image  # Skip detections without bounding boxes

        # Extract bounding box
        x1, y1, x2, y2 = det["bbox"]

        # Make sure coordinates are integers
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)

        # Check for valid box dimensions
        if x2 <= x1 or y2 <= y1:
            return image  # Skip invalid boxes

        # Create label based on configuration
        label_parts = [det["label"]]

        # Add confidence if requested
        if show_confidence and "confidence" in det:
            conf = det["confidence"]
            if isinstance(conf, (int, float)):
                label_parts.append(f"{conf:.2f}")

        # Add attributes if requested and present (limit to 2 most important)
        if show_attributes and "attributes" in det and det["attributes"]:
            # Prioritize color and size attributes
            priority_attrs = []
            for key in ["color", "size"]:
                if key in det["attributes"]:
                    priority_attrs.append(f"{key}:{det['attributes'][key]}")

            # Add up to 2 priority attributes to avoid cluttering
            if priority_attrs:
                label_parts.extend(priority_attrs[:2])

        # Add activities if present (limit to 1 most important)
        if "activities" in det and det["activities"] and len(det["activities"]) > 0:
            # Only add the first activity to avoid cluttering
            label_parts.append(f"[{det['activities'][0]}]")

        # Add "HIGHLIGHT" tag if this is a highlighted detection
        if is_highlighted:
            label_parts.append("*")

        # Combine into label
        label = " | ".join(label_parts)

        # Draw bounding box with line thickness scaled by image size
        thickness = max(
            1, min(line_thickness, int(min(image.shape[0], image.shape[1]) / 500))
        )
        cv2.rectangle(image, (x1, y1), (x2, y2), box_color, thickness)

        # Calculate text size and scale font size based on image dimensions
        font_scale = max(0.3, min(0.5, min(image.shape[0], image.shape[1]) / 1000))
        font = cv2.FONT_HERSHEY_SIMPLEX
        text_size = cv2.getTextSize(label, font, font_scale, 1)[0]

        # Draw text background with slight transparency
        alpha = 0.6  # Transparency factor
        overlay = image.copy()
        cv2.rectangle(
            overlay,
            (x1, y1 - text_size[1] - 5),
            (x1 + text_size[0], y1),
            box_color,
            -1,
        )
        cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0, image)

        # Draw text
        cv2.putText(
            image, label, (x1, y1 - 5), font, font_scale, text_color, 1
        )

        return image
```

## File: langvio/media/__init__.py

- Extension: .py
- Language: python
- Size: 0 bytes
- Created: 2025-04-01 07:38:54
- Modified: 2025-04-01 07:38:54

### Code

```python

```

## File: langvio/core/base.py

- Extension: .py
- Language: python
- Size: 1088 bytes
- Created: 2025-04-26 22:39:37
- Modified: 2025-04-26 22:39:37

### Code

```python
"""
Base classes for langvio components
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional


class Processor(ABC):
    """Base class for all processors in langvio"""

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        """
        Initialize processor.

        Args:
            name: Processor name
            config: Configuration parameters
        """
        self.name = name
        self.config = config or {}

    @abstractmethod
    def initialize(self) -> bool:
        """
        Initialize the processor with its configuration.

        Returns:
            True if initialization was successful
        """

    @classmethod
    def from_config(cls, name: str, config: Dict[str, Any]) -> "Processor":
        """
        Create a processor from configuration.

        Args:
            name: Processor name
            config: Configuration parameters

        Returns:
            Initialized processor
        """
        processor = cls(name, config)
        processor.initialize()
        return processor


```

## File: langvio/core/registry.py

- Extension: .py
- Language: python
- Size: 4230 bytes
- Created: 2025-04-20 23:47:12
- Modified: 2025-04-20 23:47:12

### Code

```python
"""
Registry for langvio models and processors
"""

from typing import Any, Dict, Type


class ModelRegistry:
    """Registry for all available models and processors"""

    def __init__(self):
        """Initialize empty registries"""
        self._llm_processors = {}
        self._vision_processors = {}

    def register_llm_processor(
        self, name: str, processor_class: Type, **kwargs
    ) -> None:
        """
        Register an LLM processor.

        Args:
            name: Name to register the processor under
            processor_class: Processor class
            **kwargs: Additional parameters to pass to the constructor
        """
        self._llm_processors[name] = (processor_class, kwargs)

    def register_vision_processor(
        self, name: str, processor_class: Type, **kwargs
    ) -> None:
        """
        Register a vision processor.

        Args:
            name: Name to register the processor under
            processor_class: Processor class
            **kwargs: Additional parameters to pass to the constructor
        """
        self._vision_processors[name] = (processor_class, kwargs)

    def get_llm_processor(self, name: str, **kwargs) -> Any:
        """
        Get an instance of an LLM processor.

        Args:
            name: Name of the registered processor
            **kwargs: Override parameters for the constructor

        Returns:
            Processor instance

        Raises:
            ValueError: If processor is not registered
        """
        if name not in self._llm_processors:
            raise ValueError(f"LLM processor '{name}' not registered")

        processor_class, default_kwargs = self._llm_processors[name]

        # Combine default kwargs with provided kwargs (provided take precedence)
        combined_kwargs = {**default_kwargs, **kwargs}

        return processor_class(**combined_kwargs)

    def get_vision_processor(self, name: str, **kwargs) -> Any:
        """
        Get an instance of a vision processor.

        Args:
            name: Name of the registered processor
            **kwargs: Override parameters for the constructor

        Returns:
            Processor instance

        Raises:
            ValueError: If processor is not registered
        """
        if name not in self._vision_processors:
            raise ValueError(f"Vision processor '{name}' not registered")

        processor_class, default_kwargs = self._vision_processors[name]

        # Combine default kwargs with provided kwargs (provided take precedence)
        combined_kwargs = {**default_kwargs, **kwargs}

        return processor_class(name, **combined_kwargs)

    def list_llm_processors(self) -> Dict[str, Type]:
        """
        List all registered LLM processors.

        Returns:
            Dictionary of processor names to processor classes
        """
        return {name: cls for name, (cls, _) in self._llm_processors.items()}

    def list_vision_processors(self) -> Dict[str, Type]:
        """
        List all registered vision processors.

        Returns:
            Dictionary of processor names to processor classes
        """
        return {name: cls for name, (cls, _) in self._vision_processors.items()}

    def register_from_entrypoints(self) -> None:
        """Load and register processors from entry points"""
        try:
            import importlib.metadata as metadata
        except ImportError:
            # Python < 3.8
            import importlib_metadata as metadata

        for ep in metadata.entry_points(group="langvio.llm_processors"):
            try:
                processor_class = ep.load()
                self.register_llm_processor(ep.name, processor_class)
            except Exception as e:
                # Log error and continue
                print(f"Error loading LLM processor {ep.name}: {e}")

        for ep in metadata.entry_points(group="langvio.vision_processors"):
            try:
                processor_class = ep.load()
                self.register_vision_processor(ep.name, processor_class)
            except Exception as e:
                # Log error and continue
                print(f"Error loading vision processor {ep.name}: {e}")

```

## File: langvio/core/__init__.py

- Extension: .py
- Language: python
- Size: 0 bytes
- Created: 2025-04-01 07:38:54
- Modified: 2025-04-01 07:38:54

### Code

```python

```

## File: langvio/core/pipeline.py

- Extension: .py
- Language: python
- Size: 12190 bytes
- Created: 2025-05-05 20:06:53
- Modified: 2025-05-05 20:06:53

### Code

```python
"""
Enhanced core pipeline for connecting LLMs with vision models
"""

import logging
import os
import sys
from typing import Any, Dict, List, Optional


from langvio.config import Config
from langvio.media.processor import MediaProcessor
from langvio.utils.file_utils import is_video_file
from langvio.utils.logging import setup_logging


class Pipeline:
    """Enhanced main pipeline for processing queries with LLMs and vision models"""

    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize pipeline.

        Args:
            config_path: Path to configuration file
        """
        # Initialize configuration
        self.config = Config(config_path)

        # Set up logging
        setup_logging(self.config.get_logging_config())
        self.logger = logging.getLogger(__name__)

        # Initialize processors
        self.llm_processor = None
        self.vision_processor = None
        self.media_processor = MediaProcessor(self.config.get_media_config())

        self.logger.info("Enhanced Pipeline initialized")

    def load_config(self, config_path: str) -> None:
        """
        Load configuration from file.

        Args:
            config_path: Path to configuration file
        """
        self.config.load_config(config_path)
        self.logger.info(f"Loaded configuration from {config_path}")

        # Reinitialize processors with new config
        if self.llm_processor:
            self.set_llm_processor(self.llm_processor.name)

        if self.vision_processor:
            self.set_vision_processor(self.vision_processor.name)

        # Update media processor
        self.media_processor.update_config(self.config.get_media_config())

    def set_llm_processor(self, processor_name: str) -> None:
        """
        Set the LLM processor.

        Args:
            processor_name: Name of the processor to use
        """
        from langvio import registry

        self.logger.info(f"Setting LLM processor to {processor_name}")

        # Get processor config
        processor_config = self.config.get_llm_config(processor_name)

        # Check if the requested processor is available
        if processor_name not in registry.list_llm_processors():
            error_msg = (
                f"ERROR: LLM processor '{processor_name}' not found. "
                "You may need to install additional dependencies:\n"
                "- For OpenAI: pip install langvio[openai]\n"
                "- For Google Gemini: pip install langvio[google]\n"
                "- For all providers: pip install langvio[all-llm]"
            )
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

        # Create processor
        try:
            self.llm_processor = registry.get_llm_processor(
                processor_name, **processor_config
            )

            # Explicitly initialize the processor
            self.llm_processor.initialize()

        except Exception as e:
            error_msg = (
                f"ERROR: Failed to initialize LLM processor '{processor_name}': {e}"
            )
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

    def set_vision_processor(self, processor_name: str) -> None:
        """
        Set the vision processor.

        Args:
            processor_name: Name of the processor to use
        """
        from langvio import registry

        self.logger.info(f"Setting vision processor to {processor_name}")

        # Get processor config
        processor_config = self.config.get_vision_config(processor_name)

        # Check if the requested processor is available
        if processor_name not in registry.list_vision_processors():
            error_msg = f"ERROR: Vision processor '{processor_name}' not found."
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

        # Create processor
        try:
            self.vision_processor = registry.get_vision_processor(
                processor_name, **processor_config
            )
        except Exception as e:
            error_msg = (
                f"ERROR: Failed to initialize vision processor '{processor_name}': {e}"
            )
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

    def process(self, query: str, media_path: str) -> Dict[str, Any]:
        """
        Process a query on media with enhanced capabilities.
        Integrates YOLO detections with YOLO11 metrics, then passes to LLM.

        Args:
            query: Natural language query
            media_path: Path to media file (image or video)

        Returns:
            Dictionary with results

        Raises:
            ValueError: If processors are not set or media file doesn't exist
        """
        self.logger.info(f"Processing query: {query}")

        # Check if processors are set
        if not self.llm_processor:
            error_msg = "ERROR: LLM processor not set"
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

        if not self.vision_processor:
            error_msg = "ERROR: Vision processor not set"
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

        # Check if media file exists
        if not os.path.exists(media_path):
            error_msg = f"ERROR: Media file not found: {media_path}"
            self.logger.error(error_msg)
            print(error_msg, file=sys.stderr)
            sys.exit(1)

        # Check media type
        is_video = is_video_file(media_path)
        media_type = "video" if is_video else "image"

        # Process query with LLM to get structured parameters
        query_params = self.llm_processor.parse_query(query)
        self.logger.info(f"Parsed query params: {query_params}")

        # Run detection with vision processor
        if is_video:
            # For video processing, check if we need to adjust sample rate based on task
            sample_rate = 5  # Default
            if query_params.get("task_type") in ["tracking", "activity"]:
                # Use a more frequent sampling for tracking and activity detection
                sample_rate = 2

            # Get all detections with YOLO11 metrics integrated
            all_detections = self.vision_processor.process_video(
                media_path, query_params, sample_rate
            )
        else:
            # Get all detections with YOLO11 metrics integrated for image
            all_detections = self.vision_processor.process_image(
                media_path, query_params
            )

        # Generate explanation using all detected objects and metrics
        explanation = self.llm_processor.generate_explanation(query, all_detections)

        # Get highlighted objects from the LLM processor
        highlighted_objects = self.llm_processor.get_highlighted_objects()

        # Create visualization with highlighted objects
        output_path = self._create_visualization(
            media_path, all_detections, highlighted_objects, query_params, is_video
        )

        # Prepare result
        result = {
            "query": query,
            "media_path": media_path,
            "media_type": media_type,
            "output_path": output_path,
            "explanation": explanation,
            "detections": all_detections,
            "query_params": query_params,
            "highlighted_objects": highlighted_objects,
        }

        self.logger.info(f"Processed query successfully")
        return result

    def _get_visualization_config(self, query_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        Get visualization configuration based on query parameters.

        Args:
            query_params: Query parameters from LLM processor

        Returns:
            Visualization configuration parameters
        """
        # Get default visualization config
        viz_config = self.config.config["media"]["visualization"].copy()

        # Customize based on task type
        task_type = query_params.get("task_type", "identification")

        if task_type == "counting":
            # For counting tasks, use a different color
            viz_config["box_color"] = [255, 0, 0]  # Red for counting

        elif task_type == "verification":
            # For verification tasks, use a different color
            viz_config["box_color"] = [0, 0, 255]  # Blue for verification

        elif task_type in ["tracking", "activity"]:
            # For tracking/activity tasks, use a more visible color
            viz_config["box_color"] = [255, 165, 0]  # Orange for tracking/activity
            viz_config["line_thickness"] = 3  # Thicker lines

        # If specific attributes were requested, adjust the visualization
        if query_params.get("attributes"):
            # If looking for specific attributes, highlight them more
            viz_config["line_thickness"] += 1

        return viz_config

    def _create_visualization(
            self,
            media_path: str,
            all_detections: Dict[str, List[Dict[str, Any]]],
            highlighted_objects: List[Dict[str, Any]],
            query_params: Dict[str, Any],
            is_video: bool,
    ) -> str:
        """
        Create visualization with all detected objects and highlighted ones.

        Args:
            media_path: Path to input media
            all_detections: All detection results
            highlighted_objects: Objects to highlight with a different color
            query_params: Query parameters
            is_video: Whether the media is a video

        Returns:
            Path to the output visualization
        """
        # Generate output path
        output_path = self.media_processor.get_output_path(media_path)

        # Get visualization config
        visualization_config = self._get_visualization_config(query_params)

        # Store original box color to use for non-highlighted objects
        original_box_color = visualization_config["box_color"]

        # Use a different, more prominent color for highlighted objects
        highlight_color = [0, 0, 255]  # Red color (BGR) for highlighted objects

        if is_video:
            # For videos, we'll pass all detections and use a different visualization approach
            # that distinguishes highlighted objects
            self.media_processor.visualize_video_with_highlights(
                media_path,
                output_path,
                all_detections,  # Use all detections instead of just highlighted ones
                highlighted_objects,  # Pass highlighted objects separately
                original_box_color=original_box_color,
                highlight_color=highlight_color,
                text_color=visualization_config["text_color"],
                line_thickness=visualization_config["line_thickness"],
                show_attributes=visualization_config.get("show_attributes", True),
                show_confidence=visualization_config.get("show_confidence", True),
            )
        else:
            # For images, we'll pass all detections and use a different visualization approach
            # that distinguishes highlighted objects
            self.media_processor.visualize_image_with_highlights(
                media_path,
                output_path,
                all_detections["0"],  # Use all detections for the single frame
                [obj["detection"] for obj in highlighted_objects],  # Extract just the detection objects
                original_box_color=original_box_color,
                highlight_color=highlight_color,
                text_color=visualization_config["text_color"],
                line_thickness=visualization_config["line_thickness"],
                show_attributes=visualization_config.get("show_attributes", True),
                show_confidence=visualization_config.get("show_confidence", True),
            )

        return output_path
```

## File: langvio/llm/base.py

- Extension: .py
- Language: python
- Size: 9160 bytes
- Created: 2025-05-05 20:37:22
- Modified: 2025-05-05 20:37:22

### Code

```python
"""
Enhanced base classes for LLM processors with expanded capabilities
"""

import importlib.util
import json
import logging
from abc import abstractmethod
from typing import Any, Dict, List, Optional

from langchain.output_parsers.json import SimpleJsonOutputParser
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langvio.core.base import Processor
from langvio.prompts.constants import TASK_TYPES
from langvio.prompts.templates import (
    EXPLANATION_TEMPLATE,
    QUERY_PARSING_TEMPLATE,
    SYSTEM_PROMPT,
)
from langvio.utils.llm_utils import (
    format_detection_summary,
    index_detections,
    parse_explanation_response,
)


class BaseLLMProcessor(Processor):
    """Enhanced base class for all LLM processors"""

    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        """Initialize LLM processor."""
        super().__init__(name, config)
        self.logger = logging.getLogger(__name__)
        self.llm = None
        self.query_chat_prompt = None
        self.explanation_chat_prompt = None

    def initialize(self) -> bool:
        """Initialize the processor with its configuration."""
        try:
            # Initialize the specific LLM implementation
            self._initialize_llm()

            # Set up prompts
            self._setup_prompts()

            return True
        except Exception as e:
            self.logger.error(f"Error initializing LLM processor: {e}")
            return False

    @abstractmethod
    def _initialize_llm(self) -> None:
        """Initialize the specific LLM implementation."""

    def _setup_prompts(self) -> None:
        """Set up the prompt templates with system message."""
        system_message = SystemMessage(content=SYSTEM_PROMPT)

        # Query parsing prompt
        self.query_chat_prompt = ChatPromptTemplate.from_messages(
            [
                system_message,
                MessagesPlaceholder(variable_name="history"),
                ("user", QUERY_PARSING_TEMPLATE),
            ]
        )

        # Explanation prompt
        self.explanation_chat_prompt = ChatPromptTemplate.from_messages(
            [
                system_message,
                MessagesPlaceholder(variable_name="history"),
                ("user", EXPLANATION_TEMPLATE),
            ]
        )

        # Create chains
        json_parser = SimpleJsonOutputParser()
        self.query_chain = self.query_chat_prompt | self.llm | json_parser
        self.explanation_chain = self.explanation_chat_prompt | self.llm

    def parse_query(self, query: str) -> Dict[str, Any]:
        """Parse a natural language query into structured parameters."""
        self.logger.info(f"Parsing query: {query}")

        try:
            # Invoke the chain with proper output parsing
            parsed = self.query_chain.invoke({"query": query, "history": []})

            # Ensure all required fields exist with defaults
            parsed = self._ensure_parsed_fields(parsed)

            # Log the parsed query
            self.logger.debug(f"Parsed query: {json.dumps(parsed, indent=2)}")

            return parsed

        except Exception as e:
            self.logger.error(f"Error parsing query: {e}")

            return {"error": e}

    def _ensure_parsed_fields(self, parsed: Dict[str, Any]) -> Dict[str, Any]:
        """Ensure all required fields exist in the parsed query."""
        defaults = {
            "target_objects": [],
            "count_objects": False,
            "task_type": "identification",
            "attributes": [],
            "spatial_relations": [],
            "activities": [],
            "custom_instructions": "",
        }

        # Add any missing fields with defaults
        for key, default_value in defaults.items():
            if key not in parsed or parsed[key] is None:
                parsed[key] = default_value

        # Ensure task_type is valid
        if parsed["task_type"] not in TASK_TYPES:
            self.logger.warning(
                f"Invalid task type: {parsed['task_type']}. Using 'identification' instead."
            )
            parsed["task_type"] = "identification"

        return parsed

    def generate_explanation(
        self, query: str, detections: Dict[str, List[Dict[str, Any]]]
    ) -> str:
        """Generate an explanation based on detection results."""
        self.logger.info("Generating explanation for detection results")

        # Get the original parsed query
        parsed_query = self.parse_query(query)

        # Add unique IDs to each detection for reference
        indexed_detections, detection_map = index_detections(detections)

        # Format detection summary with object IDs
        detection_summary = format_detection_summary(indexed_detections, parsed_query)

        # Create a summary of detections
        print(detections)
        try:
            # Invoke the explanation chain
            response = self.explanation_chain.invoke(
                {
                    "query": query,
                    "detection_summary": detection_summary,
                    "parsed_query": json.dumps(parsed_query, indent=2),
                    "history": [],
                }
            )

            # Parse the response
            if response and hasattr(response, "content"):
                explanation_text, highlight_objects = parse_explanation_response(
                    response.content, detection_map
                )

                # Store the highlighted objects for visualization
                self._highlighted_objects = highlight_objects

                return explanation_text
            else:
                return "Error generating explanation: No valid response from LLM"

        except Exception as e:
            self.logger.error(f"Error generating explanation: {e}")
            return f"Error analyzing the image: {e}"

    def get_highlighted_objects(self) -> List[Dict[str, Any]]:
        """
        Get the objects that were highlighted in the last explanation.

        Returns:
            List of highlighted objects with frame references
        """
        return getattr(self, "_highlighted_objects", [])

    def is_package_installed(self, package_name: str) -> bool:
        """Check if a Python package is installed."""
        return importlib.util.find_spec(package_name) is not None

    def parse_solution_results(solution_results):
        """
        Convert YOLO11 SolutionResults objects to well-structured dictionaries.

        Args:
            solution_results: Either object counting or speed estimation results

        Returns:
            Dictionary with structured information
        """
        # Convert to string first
        result_str = str(solution_results)

        # Create a base dictionary
        parsed_data = {}

        # Check if it's object counting results
        if "in_count" in result_str and "out_count" in result_str:
            # Extract the basic counts
            parsed_data["type"] = "object_counting"
            parsed_data["in_count"] = solution_results.in_count
            parsed_data["out_count"] = solution_results.out_count
            parsed_data["total_tracks"] = solution_results.total_tracks

            # Extract class-wise counts in a more accessible format
            class_counts = {}
            for class_name, directions in solution_results.classwise_count.items():
                # Only include classes that have non-zero counts
                if directions["IN"] > 0 or directions["OUT"] > 0:
                    class_counts[class_name] = {
                        "in": directions["IN"],
                        "out": directions["OUT"],
                        "total": directions["IN"] + directions["OUT"]
                    }

            parsed_data["class_counts"] = class_counts

            # Add a summary for quick access
            active_classes = [cls for cls, counts in class_counts.items()]
            parsed_data["summary"] = {
                "total_objects": parsed_data["in_count"] + parsed_data["out_count"],
                "active_classes": active_classes,
                "most_common_class": max(class_counts.items(), key=lambda x: x[1]["total"])[0] if class_counts else None
            }

        # Check if it's speed estimation results
        elif "total_tracks" in result_str:
            parsed_data["type"] = "speed_estimation"
            parsed_data["total_tracks"] = solution_results.total_tracks

            # If there are additional attributes in the speed results, extract them
            # This will depend on what attributes the SolutionResults object has
            if hasattr(solution_results, "track_speeds"):
                parsed_data["track_speeds"] = solution_results.track_speeds

            if hasattr(solution_results, "avg_speed"):
                parsed_data["avg_speed"] = solution_results.avg_speed

            if hasattr(solution_results, "class_speeds"):
                parsed_data["class_speeds"] = solution_results.class_speeds

        return parsed_data

```

## File: langvio/llm/google.py

- Extension: .py
- Language: python
- Size: 2419 bytes
- Created: 2025-04-20 23:39:14
- Modified: 2025-04-20 23:39:14

### Code

```python
"""
Google Gemini-specific LLM processor implementation
"""

import logging
from typing import Any, Dict, Optional

from langvio.llm.base import BaseLLMProcessor


class GeminiProcessor(BaseLLMProcessor):
    """LLM processor using Google Gemini models via LangChain"""

    def __init__(
        self,
        name: str = "gemini",
        model_name: str = "gemini-pro",
        model_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """
        Initialize Gemini processor.

        Args:
            name: Processor name
            model_name: Name of the Gemini model to use
            model_kwargs: Additional model parameters (temperature, etc.)
            **kwargs: Additional processor parameters
        """
        config = {
            "model_name": model_name,
            "model_kwargs": model_kwargs or {},
            **kwargs,
        }
        super().__init__(name, config)
        self.logger = logging.getLogger(__name__)

    def _initialize_llm(self) -> None:
        """
        Initialize the Google Gemini model via LangChain.
        This is the only method that needs to be implemented.
        """
        try:

            if not self.is_package_installed("langchain_google_genai"):
                raise ImportError(
                    "The 'langchain-google-genai' package is required to use Gemini models. "
                    "Please install it with 'pip install langvio[google]'"
                )

            # Import necessary components
            import os

            from langchain_google_genai import ChatGoogleGenerativeAI

            # Get model configuration
            model_name = self.config["model_name"]
            model_kwargs = self.config["model_kwargs"].copy()

            if "GOOGLE_API_KEY" not in os.environ:
                # Log a warning rather than setting it from config
                self.logger.warning(
                    "GOOGLE_API_KEY environment variable not found. "
                    "Please set it using 'export GOOGLE_API_KEY=your_key' o"
                )
                raise

            # Create the Gemini LLM
            self.llm = ChatGoogleGenerativeAI(model=model_name, **model_kwargs)

            self.logger.info(f"Initialized Google Gemini model: {model_name}")
        except Exception as e:
            self.logger.error(f"Error initializing Google Gemini model: {e}")
            raise

```

## File: langvio/llm/openai.py

- Extension: .py
- Language: python
- Size: 2373 bytes
- Created: 2025-04-20 23:39:14
- Modified: 2025-04-20 23:39:14

### Code

```python
"""
OpenAI-specific LLM processor implementation
"""

import logging
from typing import Any, Dict, Optional

from langvio.llm.base import BaseLLMProcessor


class OpenAIProcessor(BaseLLMProcessor):
    """LLM processor using OpenAI models via LangChain"""

    def __init__(
        self,
        name: str = "openai",
        model_name: str = "gpt-3.5-turbo",
        model_kwargs: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """
        Initialize OpenAI processor.

        Args:
            name: Processor name
            model_name: Name of the OpenAI model to use
            model_kwargs: Additional model parameters (temperature, etc.)
            **kwargs: Additional processor parameters
        """
        config = {
            "model_name": model_name,
            "model_kwargs": model_kwargs or {},
            **kwargs,
        }
        super().__init__(name, config)
        self.logger = logging.getLogger(__name__)

    def _initialize_llm(self) -> None:
        """
        Initialize the OpenAI model via LangChain.
        This is the only method that needs to be implemented.
        """
        try:
            if not self.is_package_installed("langchain_openai"):
                raise ImportError(
                    "The 'langchain-openai' package is required to use OpenAI models. "
                    "Please install it with 'pip install langvio[openai]'"
                )

            # Import necessary components
            import os

            from langchain_openai import ChatOpenAI

            # Get model configuration
            model_name = self.config["model_name"]
            model_kwargs = self.config["model_kwargs"].copy()

            if "OPENAI_API_KEY" not in os.environ:
                # Log a warning rather than setting it from config
                self.logger.warning(
                    "OPENAI_API_KEY environment variable not found. "
                    "Please set it using 'export OPENAI_API_KEY=your_key' "
                )
                raise
            else:
                # Create the OpenAI LLM
                self.llm = ChatOpenAI(model_name=model_name, **model_kwargs)

            self.logger.info(f"Initialized OpenAI model: {model_name}")
        except Exception as e:
            self.logger.error(f"Error initializing OpenAI model: {e}")
            raise

```

## File: langvio/llm/__init__.py

- Extension: .py
- Language: python
- Size: 314 bytes
- Created: 2025-04-20 23:38:42
- Modified: 2025-04-20 23:38:42

### Code

```python
"""
LLM processors module for langvio
"""

from langvio.llm.base import BaseLLMProcessor

# These imports will be done dynamically by the factory
# to avoid requiring all dependencies
# from langvio.llm.openai import OpenAIProcessor
# from langvio.llm.google import GeminiProcessor

__all__ = ["BaseLLMProcessor"]

```

## File: langvio/llm/factory.py

- Extension: .py
- Language: python
- Size: 3211 bytes
- Created: 2025-04-20 23:47:12
- Modified: 2025-04-20 23:47:12

### Code

```python
"""
Factory for LLM processor registration and creation
"""

import importlib.util
import logging
import sys

from langvio.core.registry import ModelRegistry

logger = logging.getLogger(__name__)


def register_llm_processors(registry: ModelRegistry) -> None:
    """
    Register available LLM processors with the registry.
    This function performs lazy loading to avoid unnecessary imports.
    If no LLM providers are available, it will log an error but NOT register a fallback.

    Args:
        registry: The model registry to register processors with
    """
    llm_providers_found = False

    # Register OpenAI processor if available
    if is_package_available("langchain_openai"):
        try:
            from langvio.llm.openai import OpenAIProcessor

            # Register GPT-3.5 model
            registry.register_llm_processor(
                "gpt-3.5",
                OpenAIProcessor,
                model_name="gpt-3.5-turbo",
                model_kwargs={"temperature": 0.2},
            )

            # Register GPT-4 model
            registry.register_llm_processor(
                "gpt-4",
                OpenAIProcessor,
                model_name="gpt-4-turbo",
                model_kwargs={"temperature": 0.2},
            )

            llm_providers_found = True
            logger.info("Registered OpenAI LLM processors")
        except Exception as e:
            logger.warning(f"Failed to register OpenAI processors: {e}")

    # Register Google Gemini processor if available
    if is_package_available("langchain_google_genai"):
        try:
            from langvio.llm.google import GeminiProcessor

            # # Register Gemini Pro model
            registry.register_llm_processor(
                "gemini",
                GeminiProcessor,
                model_name="gemini-pro",
                model_kwargs={"temperature": 0.2},
            )

            # Set as default if available
            registry.register_llm_processor(
                "default",
                GeminiProcessor,
                model_name="gemini-pro",
                model_kwargs={"temperature": 0.2},
            )

            llm_providers_found = True
            logger.info("Registered Google Gemini LLM processors")
        except Exception as e:
            logger.warning(f"Failed to register Google Gemini processors: {e}")

    # If no LLM providers are available, log an error but DO NOT register a fallback
    if not llm_providers_found:
        error_msg = (
            "ERROR: No LLM providers are installed. Please install at least one provider:\n"
            "- For OpenAI: pip install langvio[openai]\n"
            "- For Google Gemini: pip install langvio[google]\n"
            "- For all providers: pip install langvio[all-llm]"
        )
        logger.error(error_msg)
        # print(error_msg, file=sys.stderr)
        sys.exit(1)


def is_package_available(package_name: str) -> bool:
    """
    Check if a Python package is available.

    Args:
        package_name: Name of the package to check

    Returns:
        True if the package is available, False otherwise
    """
    return importlib.util.find_spec(package_name) is not None

```

## File: langvio/prompts/templates.py

- Extension: .py
- Language: python
- Size: 5143 bytes
- Created: 2025-05-05 20:12:03
- Modified: 2025-05-05 20:12:03

### Code

```python
"""
Enhanced prompt templates for LLM processors
"""

# Query parsing prompt template with extended capabilities
QUERY_PARSING_TEMPLATE = """
Translate the following natural language query about images/videos into structured commands for an object detection
and analysis system.

Query: {query}

The JSON response must have the following fields:

- target_objects: List of object categories to detect.
- count_objects: Boolean indicating if counting is needed
- task_type: One of "identification", "counting", "verification", "analysis", "tracking", "activity"
- attributes: List of dictionaries for attributes to check, e.g. "attribute": "color", "value": "red"
- spatial_relations: List of dictionaries for spatial relationships, e.g. "relation": "above", "object": "table"
- activities: List of activities to detect (for videos), e.g. "walking", "running"
- custom_instructions: Any additional processing instructions that don't fit the categories above

Be precise and thorough in interpreting the query.
"""

EXPLANATION_TEMPLATE = """
Based on the user's query and detection results, provide a response in TWO clearly separated sections.

User query: {query}

Detection results: {detection_summary}

Query parsed as: {parsed_query}

Your response MUST have these two sections:

EXPLANATION:
Provide a clear, helpful explanation that directly addresses the user's query based on what was detected.
Focus on answering their specific question or fulfilling their request.
If the user asked about attributes, spatial relationships, or activities, include that information.
If objects were not found, explain what was searched for but not found.
For counts, provide exact numbers.
For verification queries, explicitly confirm or deny what was asked.
Structure the response in a natural, conversational way.
This section will be shown to the user.
Make sure that explanation is quite natural explaining a image/video to a person.

HIGHLIGHT_OBJECTS:
List the exact object_ids of objects that should be highlighted in the visualization.
Only include objects that you directly mention in your explanation.
Format this as a JSON array of strings, e.g. ["obj_0", "obj_3", "obj_5"]
This section will NOT be shown to the user but will be used to create the visualization.
"""


# Enhanced system prompt with object highlighting capabilities
SYSTEM_PROMPT = """
You are an AI assistant that helps analyze visual content using natural language.

You have three main tasks:
1. Parse natural language queries into structured commands for object detection and analysis
2. Generate explanations of detection results
3. Select specific objects to highlight in visualizations

For parsing queries, you need to extract:
- Target objects to detect
- Whether objects should be counted
- The type of analysis needed (identification, counting, verification, etc.)
- Any attributes to check (color, size, etc.)
- Any spatial relationships to analyze (above, below, next to, etc.)
- Any activities to detect (for videos)

When asked to parse a query, you must respond with VALID JSON ONLY - no explanations or extra text.

When generating explanations, your response MUST have two clearly separated sections:
1. EXPLANATION: Your user-friendly explanation (this will be shown to the user)
2. HIGHLIGHT_OBJECTS: A list of object_ids to highlight (this will be removed from the user-facing response)

The HIGHLIGHT_OBJECTS section MUST:
- Be clearly separated from the EXPLANATION section with "HIGHLIGHT_OBJECTS:" on its own line
- Use the exact object_ids from the detection results (like obj_0, obj_1)
- Include only objects that are directly mentioned in your explanation
- Be formatted as a JSON array of strings, e.g. ["obj_0", "obj_3", "obj_5"]

EXAMPLES:

Query parsing example:
Input: "Find all the cars in this image"
Output: {
  "target_objects": ["car"],
  "count_objects": true,
  "task_type": "identification",
  "attributes": [],
  "spatial_relations": [],
  "activities": [],
  "custom_instructions": ""
}

Explanation example:
Input query: "How many people are in this image?"
Detection results:
- [obj_0] person (confidence: 0.95) - position: center-right
- [obj_1] person (confidence: 0.87) - position: bottom-left
- [obj_2] dog (confidence: 0.92) - position: bottom-right

Output:
EXPLANATION:
I detected 2 people in the image. One person is positioned in the center-right area, while the other is in the bottom-left corner.

HIGHLIGHT_OBJECTS:
["obj_0", "obj_1"]

Input query: "Are there any red objects in this image?"
Detection results:
- [obj_0] car (confidence: 0.95) - color:red, position: center-left
- [obj_1] car (confidence: 0.87) - color:blue, position: top-right
- [obj_2] book (confidence: 0.92) - color:red, position: bottom-right

Output:
EXPLANATION:
Yes, there are two red objects in the image: a red car in the center-left area and a red book in the bottom-right corner. There's also a blue car in the top-right.

HIGHLIGHT_OBJECTS:
["obj_0", "obj_2"]

Remember: The EXPLANATION section will be shown to the user, but the HIGHLIGHT_OBJECTS section will be used only for visualization and removed from the final response.
"""

```

## File: langvio/prompts/__init__.py

- Extension: .py
- Language: python
- Size: 240 bytes
- Created: 2025-04-20 23:45:02
- Modified: 2025-04-20 23:45:02

### Code

```python
"""
Prompt templates package for langvio
"""

from langvio.prompts.templates import (
    EXPLANATION_TEMPLATE,
    QUERY_PARSING_TEMPLATE,
    SYSTEM_PROMPT,
)

__all__ = ["QUERY_PARSING_TEMPLATE", "EXPLANATION_TEMPLATE", "SYSTEM_PROMPT"]

```

## File: langvio/prompts/constants.py

- Extension: .py
- Language: python
- Size: 725 bytes
- Created: 2025-05-17 14:05:14
- Modified: 2025-05-17 14:05:14

### Code

```python
"""
Constants for langvio package
"""

# Task types that the system can handle
TASK_TYPES = [
    "identification",  # Basic object detection
    "counting",  # Counting specific objects
    "verification",  # Verifying existence of objects
    "analysis",  # Detailed analysis with attributes and relationships
    "tracking",  # For tracking objects across video frames
    "activity",  # For detecting activities/actions
]

# Default detection confidence threshold
DEFAULT_CONFIDENCE_THRESHOLD = 0.25

# Default IoU threshold for NMS
DEFAULT_IOU_THRESHOLD = 0.5

# Default sample rate for video processing (every N frames)
DEFAULT_VIDEO_SAMPLE_RATE = 2

YOLO11_CONFIG = {
"model_path": "yolo11n.pt" ,
"confidence" : 0.5
}

```

## File: langvio/vision/yolo/detector.py

- Extension: .py
- Language: python
- Size: 19422 bytes
- Created: 2025-05-17 13:57:46
- Modified: 2025-05-17 13:57:46

### Code

```python
"""
Simplified YOLO-based vision processor with integrated YOLOe and YOLO11 processing
"""

import logging
import os
from typing import Any, Dict, List, Optional, Tuple

import cv2
import numpy as np
import torch
from ultralytics import YOLO, YOLOE

from langvio.prompts.constants import DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_VIDEO_SAMPLE_RATE
from langvio.vision.base import BaseVisionProcessor
from langvio.vision.utils import extract_detections, optimize_for_memory, calculate_relative_positions, detect_spatial_relationships
from langvio.vision.yolo.yolo11_utils import (
    check_yolo11_solutions_available,
    create_object_counter,
    create_speed_estimator,
    process_frame_with_yolo11,
    parse_solution_results, initialize_yolo11_tools
)


class YOLOProcessor(BaseVisionProcessor):
    """Simplified vision processor using YOLO models with integrated YOLO11 metrics"""

    def __init__(self, name: str, model_path: str, confidence: float = DEFAULT_CONFIDENCE_THRESHOLD, **kwargs):
        """
        Initialize YOLO processor.

        Args:
            name: Processor name
            model_path: Path to the YOLO model
            confidence: Confidence threshold for detections
            **kwargs: Additional parameters for YOLO
        """
        config = {
            "model_path": model_path,
            "confidence": confidence,
            **kwargs,
        }
        super().__init__(name, config)
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.model_type = kwargs.get("model_type", "yolo")

        # Check YOLO11 Solutions availability
        self.has_yolo11_solutions = check_yolo11_solutions_available()
        if self.has_yolo11_solutions:
            self.logger.info("YOLO11 Solutions is available for metrics")
        else:
            self.logger.info("YOLO11 Solutions not available - using basic detection only")

    def initialize(self) -> bool:
        """
        Initialize the YOLO model.

        Returns:
            True if initialization was successful
        """
        try:
            self.logger.info(f"Loading {self.model_type} model: {self.config['model_path']}")

            # Use YOLOE by default for better performance
            if self.model_type == "yoloe":
                self.model = YOLOE(self.config['model_path'])
            else:
                self.model = YOLO(self.config['model_path'])

            return True
        except Exception as e:
            self.logger.error(f"Error loading YOLO model: {e}")
            return False

    def initialize_video_capture(self, video_path: str) -> Tuple[cv2.VideoCapture, Tuple[int, int, float, int]]:
        """
        Initialize video capture and extract video properties.

        Args:
            video_path: Path to the video file

        Returns:
            Tuple containing (video_capture, (width, height, fps, total_frames))

        Raises:
            ValueError: If video cannot be opened
        """
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Failed to open video: {video_path}")

        # Get video properties
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 25.0  # Default FPS
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        return cap, (width, height, fps, total_frames)

    def process_image(self, image_path: str, query_params: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """
        Process an image with YOLOe with enhanced detection capabilities.
        For images: only uses YOLOe with attribute computation.

        Args:
            image_path: Path to the input image
            query_params: Parameters from the query processor

        Returns:
            Dictionary with detection results and attributes
        """
        self.logger.info(f"Processing image: {image_path}")

        # Load model if not already loaded
        if not self.model:
            self.initialize()

        with torch.no_grad():
            try:
                optimize_for_memory()

                # Get image dimensions for relative positioning
                image_dimensions = self._get_image_dimensions(image_path)

                # Run basic object detection with YOLOe
                results = self.model(image_path, conf=self.config["confidence"])

                # Extract detections
                detections = extract_detections(results)

                # Add position, size and color attributes to detections
                detections = self._enhance_detections_with_attributes(detections, image_path)

                # Calculate relative positions
                if image_dimensions:
                    detections = calculate_relative_positions(detections, *image_dimensions)
                    detections = detect_spatial_relationships(detections)

                # Create result with detections
                frame_detections = {"0": detections}

                # Create a simple summary of detections
                frame_detections["summary"] = self._create_simple_summary(detections)

                return frame_detections

            except Exception as e:
                self.logger.error(f"Error processing image: {e}")
                import traceback
                self.logger.error(traceback.format_exc())
                return {"0": [], "error": str(e)}

    def process_video(
            self,
            video_path: str,
            query_params: Dict[str, Any],
            sample_rate: int = DEFAULT_VIDEO_SAMPLE_RATE,
            show_frame: bool = False
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        Process a video in a single loop, running both YOLOe and YOLO11 on each frame.

        Args:
            video_path: Path to the input video
            query_params: Parameters from the query processor
            sample_rate: Process every Nth frame

        Returns:
            Dictionary with combined detection results and metrics
        """
        self.logger.info(f"Processing video: {video_path}")

        # Load model if not already loaded
        if not self.model:
            self.initialize()

        # Initialize results container
        frame_detections = {}
        counter_results = None
        speed_results = None
        cap = None

        try:
            # 1. Initialize video and auxiliary tools
            cap, video_props = self.initialize_video_capture(video_path)
            width, height, fps, total_frames = video_props
            counter, speed_estimator = initialize_yolo11_tools(width, height)

            # 2. Process all frames
            frame_idx = 0
            with torch.no_grad():
                while frame_idx < total_frames:
                    # Read frame
                    success, frame = cap.read()
                    if not success:
                        break

                    cv2.imshow("Video Frame", frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break


                    # Process every Nth frame based on sample_rate
                    if frame_idx % sample_rate == 0:
                        result = self._process_single_frame(
                            frame, frame_idx, total_frames,
                            width, height,
                            counter, speed_estimator
                        )


                        # Extract results
                        detections = result.get("detections", [])

                        for det in detections:
                            if "bbox" not in det or "label" not in det:
                                continue

                            x1, y1, x2, y2 = map(int, det["bbox"])
                            label = det["label"]
                            color = (0, 255, 0)  # Green bounding box

                            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                                        0.6, color, 2)

                        frame_detections[str(frame_idx)] = detections

                        # Update metrics if available
                        if "counter_result" in result and result["counter_result"]:
                            counter_results = result["counter_result"]
                        if "speed_result" in result and result["speed_result"]:
                            speed_results = result["speed_result"]

                    # Increment frame counter
                    frame_idx += 1

            # 3. Create summary from results
            return self._finalize_results(frame_detections, counter_results, speed_results)

        except Exception as e:
            self.logger.error(f"Error processing video: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {"error": str(e)}
        finally:
            # Ensure resources are properly released
            if cap is not None:
                cap.release()

    def _process_single_frame(
            self,
            frame: np.ndarray,
            frame_idx: int,
            total_frames: int,
            width: int,
            height: int,
            counter: Any,
            speed_estimator: Any
    ) -> Dict[str, Any]:
        """
        Process a single video frame with both YOLOe and YOLO11.

        Args:
            frame: The video frame to process
            frame_idx: Current frame index
            total_frames: Total number of frames
            width: Video width
            height: Video height
            counter: YOLO11 counter object or None
            speed_estimator: YOLO11 speed estimator object or None

        Returns:
            Dictionary with frame processing results
        """
        result = {
            "detections": [],
            "counter_result": None,
            "speed_result": None
        }

        try:
            self.logger.info(f"Processing frame {frame_idx}/{total_frames}")

            # 1. Run basic object detection with YOLOe
            model_results = self.model(frame, conf=self.config["confidence"])

            # 2. Extract and enhance detections
            detections = extract_detections(model_results)

            if len(detections) > 0:
                # Add position, size and color attributes to detections
                detections = self._enhance_detections_with_attributes(detections, None, frame)

                # Calculate relative positions
                detections = calculate_relative_positions(detections, width, height)
                detections = detect_spatial_relationships(detections)

            result["detections"] = detections

            # 3. Run YOLO11 metrics if available
            if self.has_yolo11_solutions and (counter or speed_estimator):
                try:
                    counter_frame, speed_frame = process_frame_with_yolo11(
                        frame, counter, speed_estimator
                    )
                    result["counter_result"] = counter_frame
                    result["speed_result"] = speed_frame
                except Exception as e:
                    self.logger.warning(f"YOLO11 processing error on frame {frame_idx}: {e}")

        except Exception as e:
            self.logger.warning(f"Error processing frame {frame_idx}: {e}")
            # We return partial results instead of raising to continue processing

        return result

    def _finalize_results(
            self,
            frame_detections: Dict[str, List[Dict[str, Any]]],
            counter_results: Any,
            speed_results: Any
    ) -> Dict[str, Any]:
        """
        Finalize and structure the results with metrics and summaries.

        Args:
            frame_detections: Dictionary mapping frame indices to detections
            counter_results: Results from the YOLO11 counter
            speed_results: Results from the YOLO11 speed estimator

        Returns:
            Structured results dictionary
        """
        # Parse YOLO11 metrics for easier use by LLM
        metrics = {}
        if counter_results:
            metrics["counting"] = parse_solution_results(counter_results)
        if speed_results:
            metrics["speed"] = parse_solution_results(speed_results)

        # Add metrics to result if available
        if metrics:
            frame_detections["metrics"] = metrics
            frame_detections["summary"] = self._create_combined_summary(frame_detections, metrics)
        else:
            # Create basic summary without metrics
            frame_detections["summary"] = self._create_simple_summary(frame_detections)

        return frame_detections

    def _enhance_detections_with_attributes(
        self,
        detections: List[Dict[str, Any]],
        image_path: Optional[str] = None,
        image: Optional[Any] = None
    ) -> List[Dict[str, Any]]:
        """
        Enhance detections with size and color attributes.

        Args:
            detections: List of detection dictionaries
            image_path: Path to the image (optional)
            image: Direct image array for video frames (optional)

        Returns:
            Detections with added size and color attributes
        """
        # Load image if path is provided and image is not
        if image_path and image is None:
            try:
                image = cv2.imread(image_path)
            except Exception as e:
                self.logger.error(f"Error loading image for attribute detection: {e}")
                return detections

        # If we don't have an image, return unchanged detections
        if image is None:
            return detections

        image_height, image_width = image.shape[:2]

        for det in detections:
            # Skip if no bbox
            if "bbox" not in det:
                continue

            # Extract bounding box
            x1, y1, x2, y2 = det["bbox"]

            # Skip invalid boxes
            if (x1 >= x2 or y1 >= y2 or x1 < 0 or y1 < 0 or
                x2 > image_width or y2 > image_height):
                continue

            # Get the object region
            obj_region = image[int(y1):int(y2), int(x1):int(x2)]

            # Initialize attributes dictionary if not present
            if "attributes" not in det:
                det["attributes"] = {}

            # Calculate size attribute
            area = (x2 - x1) * (y2 - y1)
            image_area = image_width * image_height
            relative_size = area / image_area

            if relative_size < 0.05:
                det["attributes"]["size"] = "small"
            elif relative_size < 0.25:
                det["attributes"]["size"] = "medium"
            else:
                det["attributes"]["size"] = "large"

            # Extract dominant color
            if obj_region.size > 0:
                # Get color information
                from langvio.vision.color_detection import ColorDetector
                color_info = ColorDetector.get_color_profile(obj_region)

                # Add to detection attributes
                det["attributes"]["color"] = color_info["dominant_color"]
                det["attributes"]["is_multicolored"] = color_info["is_multicolored"]

        return detections

    def _create_simple_summary(self, detections) -> Dict[str, Any]:
        """
        Create a simple summary from basic YOLOe detections.

        Args:
            detections: Either a list of detections (for images) or
                        a dictionary of frames with detections (for videos)

        Returns:
            Summary dictionary
        """
        # For a single image (list of detections)
        if isinstance(detections, list):
            # Count objects by type
            counts = {}
            for det in detections:
                label = det["label"]
                if label not in counts:
                    counts[label] = 0
                counts[label] += 1

            return {
                "counts": counts,
                "total_objects": len(detections),
                "object_types": list(counts.keys())
            }

        # For a video (dictionary of frames)
        elif isinstance(detections, dict):
            # Count objects by type across all frames
            counts = {}
            track_ids = set()

            # Go through all frame keys
            for frame_key, frame_detections in detections.items():
                if not frame_key.isdigit():  # Skip non-frame keys
                    continue

                for det in frame_detections:
                    label = det["label"]
                    if label not in counts:
                        counts[label] = 0
                    counts[label] += 1

                    # Track unique objects by track_id
                    if "track_id" in det:
                        track_ids.add((label, det["track_id"]))

            # Count unique objects by type
            unique_by_type = {}
            for label, _ in track_ids:
                if label not in unique_by_type:
                    unique_by_type[label] = 0
                unique_by_type[label] += 1

            return {
                "counts": counts,
                "total_frames_analyzed": len([k for k in detections.keys() if k.isdigit()]),
                "total_detections": sum(counts.values()),
                "unique_tracked_objects": len(track_ids),
                "unique_by_type": unique_by_type,
                "object_types": list(counts.keys())
            }

        # Default empty summary
        return {}

    def _create_combined_summary(self, frame_detections: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
        """
        Create a combined summary that merges YOLOe detections with YOLO11 metrics.

        Args:
            frame_detections: Dictionary with frame-by-frame detections
            metrics: Dictionary with YOLO11 metrics

        Returns:
            Combined summary dictionary
        """
        # Get basic summary from YOLOe
        summary = self._create_simple_summary(frame_detections)

        # Add YOLO11 metrics if available
        if metrics:
            if "counting" in metrics:
                counting = metrics["counting"]
                if "summary" in counting:
                    summary["counting_summary"] = counting["summary"]
                if "in_count" in counting and "out_count" in counting:
                    summary["in_count"] = counting["in_count"]
                    summary["out_count"] = counting["out_count"]
                if "class_counts" in counting:
                    summary["class_direction_counts"] = counting["class_counts"]

            if "speed" in metrics:
                speed = metrics["speed"]
                if "total_tracks" in speed:
                    summary["tracked_objects_with_speed"] = speed["total_tracks"]
                if "avg_speed" in speed:
                    summary["average_speed"] = speed["avg_speed"]
                if "class_speeds" in speed:
                    summary["class_speeds"] = speed["class_speeds"]

        return summary
```

## File: langvio/vision/yolo/__init__.py

- Extension: .py
- Language: python
- Size: 0 bytes
- Created: 2025-04-01 07:38:54
- Modified: 2025-04-01 07:38:54

### Code

```python

```

## File: langvio/vision/yolo/yolo11_utils.py

- Extension: .py
- Language: python
- Size: 7322 bytes
- Created: 2025-05-17 13:30:53
- Modified: 2025-05-17 13:30:53

### Code

```python
"""
YOLO11 utilities for object counting, speed estimation, and more
"""

import logging
from typing import Any, Dict, Optional, Tuple

from langvio.prompts.constants import YOLO11_CONFIG

logger = logging.getLogger(__name__)


def check_yolo11_solutions_available() -> bool:
    """
    Check if YOLO11 Solutions is available.

    Returns:
        True if available, False otherwise
    """
    try:
        from ultralytics import solutions
        return True
    except (ImportError, AttributeError):
        logger.warning("YOLO11 Solutions not available. Install with: pip install ultralytics>=8.0.0")
        return False


def initialize_yolo11_tools(width: int, height: int) -> Tuple[Any, Any]:
    """
    Initialize YOLO11 Solutions tools if available.

    Args:
        width: Video width
        height: Video height

    Returns:
        Tuple containing (counter, speed_estimator) - may be None if not available
    """
    counter = None
    speed_estimator = None
    has_yolo11_solutions=False

    # Check if YOLO11 Solutions are available
    has_yolo11_solutions = check_yolo11_solutions_available()

    if has_yolo11_solutions:
        try:
            yolo11_config = YOLO11_CONFIG
            # Create counter with region covering the full frame
            counter = create_object_counter(
                model_path=yolo11_config["model_path"],
                confidence=yolo11_config["confidence"],
                region=[(0, 0), (width, 0), (width, height), (0, height)]
            )

            # Create speed estimator
            speed_estimator = create_speed_estimator(
                model_path=yolo11_config["model_path"],
                confidence=yolo11_config["confidence"],
                region_width=width
            )
        except Exception as e:
            logger.warning(f"Failed to initialize YOLO11 tools: {e}")
            has_yolo11_solutions = False

    return counter, speed_estimator

def create_object_counter(model_path: str, confidence: float, region: Optional[list] = None):
    """
    Create a YOLO11 ObjectCounter.

    Args:
        model_path: Path to the YOLO model
        confidence: Confidence threshold
        region: Optional counting region coordinates [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]

    Returns:
        ObjectCounter object or None if not available
    """
    if not check_yolo11_solutions_available():
        return None

    try:
        from ultralytics import solutions

        if region is None:
            # Default region covers entire frame and will be adjusted in first frame
            region = [(0, 0), (100, 0), (100, 100), (0, 100)]

        counter = solutions.ObjectCounter(
            model=model_path,
            region=region,
            classes=[],  # Count all classes
            conf=confidence,
            verbose=False,
            show=False
        )

        return counter
    except Exception as e:
        logger.error(f"Error creating object counter: {e}")
        return None


def create_speed_estimator(model_path: str, confidence: float, region_width: Optional[int] = None):
    """
    Create a YOLO11 SpeedEstimator.

    Args:
        model_path: Path to the YOLO model
        confidence: Confidence threshold
        region_width: Optional width of the region in pixels

    Returns:
        SpeedEstimator object or None if not available
    """
    if not check_yolo11_solutions_available():
        return None

    try:
        from ultralytics import solutions

        # Create speed estimator
        speed_estimator = solutions.SpeedEstimator(
            model=model_path,
            conf=confidence,
            show=False,
            verbose=False
        )

        return speed_estimator
    except Exception as e:
        logger.error(f"Error creating speed estimator: {e}")
        return None


def process_frame_with_yolo11(frame, counter, speed_estimator):
    """
    Process a single frame with YOLO11 Solutions.

    Args:
        frame: CV2 frame
        counter: Object counter instance
        speed_estimator: Speed estimator instance

    Returns:
        Tuple of (counter_results, speed_results)
    """
    counter_results = None
    speed_results = None

    try:
        # Update counter if available
        if counter:
            counter_results = counter(frame.copy())

        # Update speed estimator if available
        if speed_estimator:
            speed_results = speed_estimator(frame.copy())

    except Exception as e:
        logger.error(f"Error processing frame with YOLO11: {e}")

    return counter_results, speed_results


def parse_solution_results(solution_results: Any) -> Dict[str, Any]:
    """
    Convert YOLO11 SolutionResults objects to well-structured dictionaries.

    Args:
        solution_results: Either object counting or speed estimation results
            from YOLO11 Solutions

    Returns:
        Dictionary with structured information
    """
    # If no results or not a proper object, return empty dict
    if not solution_results:
        return {}

    # Create a base dictionary
    parsed_data = {}

    # Check if it's object counting results
    if hasattr(solution_results, "in_count") and hasattr(solution_results, "out_count"):
        # Extract the basic counts
        parsed_data["type"] = "object_counting"
        parsed_data["in_count"] = solution_results.in_count
        parsed_data["out_count"] = solution_results.out_count
        parsed_data["total_tracks"] = solution_results.total_tracks

        # Extract class-wise counts in a more accessible format
        class_counts = {}
        if hasattr(solution_results, "classwise_count"):
            for class_name, directions in solution_results.classwise_count.items():
                # Only include classes that have non-zero counts
                if directions["IN"] > 0 or directions["OUT"] > 0:
                    class_counts[class_name] = {
                        "in": directions["IN"],
                        "out": directions["OUT"],
                        "total": directions["IN"] + directions["OUT"]
                    }

        parsed_data["class_counts"] = class_counts

        # Add a summary for quick access
        active_classes = list(class_counts.keys())
        most_common = None
        if class_counts:
            most_common = max(class_counts.items(), key=lambda x: x[1]["total"])[0]

        parsed_data["summary"] = {
            "total_objects": parsed_data["in_count"] + parsed_data["out_count"],
            "active_classes": active_classes,
            "most_common_class": most_common
        }

    # Check if it's speed estimation results
    elif hasattr(solution_results, "total_tracks"):
        parsed_data["type"] = "speed_estimation"
        parsed_data["total_tracks"] = solution_results.total_tracks

        # If there are additional attributes in the speed results, extract them
        if hasattr(solution_results, "track_speeds"):
            parsed_data["track_speeds"] = solution_results.track_speeds

        if hasattr(solution_results, "avg_speed"):
            parsed_data["avg_speed"] = solution_results.avg_speed

        if hasattr(solution_results, "class_speeds"):
            parsed_data["class_speeds"] = solution_results.class_speeds

    return parsed_data
```

